{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-27T03:58:09.855624Z",
     "start_time": "2022-05-27T03:58:09.839300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import os\\nfrom tqdm.notebook import tqdm\\nfiles = tuple()\\nwith open(\"files.txt\", \"w\") as f:\\n    for path, currentDirectory, files in tqdm(os.walk(\"trial-data\")):\\n        for file in files:\\n            if file.endswith(\"Grobid-out.txt\"):\\n                a = str(os.path.join(path, file))\\n                #print(a)\\n                f.write(a + \\'\\n\\')\\n                #shutil.copy(os.path.abspath(file), \\'trial-data/papers\\')\\n                '"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import os\n",
    "from tqdm.notebook import tqdm\n",
    "files = tuple()\n",
    "with open(\"files.txt\", \"w\") as f:\n",
    "    for path, currentDirectory, files in tqdm(os.walk(\"trial-data\")):\n",
    "        for file in files:\n",
    "            if file.endswith(\"Grobid-out.txt\"):\n",
    "                a = str(os.path.join(path, file))\n",
    "                #print(a)\n",
    "                f.write(a + '\\n')\n",
    "                #shutil.copy(os.path.abspath(file), 'trial-data/papers')\n",
    "                '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-27T04:31:34.773492Z",
     "start_time": "2022-05-27T04:31:34.722809Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['trial-data\\\\machine-translation\\\\0\\\\1406.1078v3-Grobid-out.txt',\n",
       " 'trial-data\\\\machine-translation\\\\1\\\\1610.10099v2-Grobid-out.txt',\n",
       " 'trial-data\\\\machine-translation\\\\2\\\\1706.03762v5-Grobid-out.txt',\n",
       " 'trial-data\\\\machine-translation\\\\3\\\\1606.04199v3-Grobid-out.txt',\n",
       " 'trial-data\\\\machine-translation\\\\4\\\\1804.09057v1-Grobid-out.txt']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('files.txt') as file:\n",
    "    files = file.readlines()\n",
    "    files = [i.rstrip() for i in files]\n",
    "files[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-27T04:31:36.708305Z",
     "start_time": "2022-05-27T04:31:36.692356Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_intro(a):\n",
    "    for i in a:\n",
    "        if i == 'Introduction\\n':\n",
    "            a = a[a.index(i) +1:]\n",
    "\n",
    "    intro = \"\"\n",
    "    for i in a:\n",
    "        intro += i.rstrip()\n",
    "        if len(i) <= 20:\n",
    "            break\n",
    "    \n",
    "    return intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-27T04:31:36.939764Z",
     "start_time": "2022-05-27T04:31:36.857535Z"
    }
   },
   "outputs": [],
   "source": [
    "intros = []\n",
    "for i in range(len(files)):\n",
    "    with open(files[i]) as file:\n",
    "        paper_text = file.readlines()\n",
    "        #print(paper_text)\n",
    "    intro = get_intro(paper_text)\n",
    "    intros.append(intro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-27T04:31:37.281496Z",
     "start_time": "2022-05-27T04:31:37.253468Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intros = list(filter(('title').__ne__, intros))\n",
    "intros.count('title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-27T04:31:38.234743Z",
     "start_time": "2022-05-27T04:31:38.152066Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep neural networks have shown great success in various applications such as objection recognition (see, e.g.,) and speech recognition (see, e.g.,). Furthermore, many recent works showed that neural networks can be successfully used in a number of tasks in natural language processing (NLP). These include, but are not limited to, language modeling, paraphrase detection and word embedding extraction. In the field of statistical machine translation (SMT), deep neural networks have begun to show promising results. summarizes a successful usage of feedforward neural networks in the framework of phrase-based SMT system. Along this line of research on using neural networks for SMT, this paper focuses on a novel neural network architecture that can be used as apart of the conventional phrase-based SMT system. The proposed neural network architecture, which we will refer to as an RNN Encoder-Decoder, consists of two recurrent neural networks (RNN) that act as an encoder and a decoder pair. The encoder maps a variable-length source sequence to a fixed-length vector, and the decoder maps the vector representation back to a variable-length target sequence. The two networks are trained jointly to maximize the conditional probability of the target sequence given a source sequence. Additionally, we propose to use a rather sophisticated hidden unit in order to improve both the memory capacity and the ease of training.The proposed RNN Encoder-Decoder with a novel hidden unit is empirically evaluated on the task of translating from English to French. We train the model to learn the translation probability of an English phrase to a corresponding French phrase. The model is then used as apart of a standard phrase-based SMT system by scoring each phrase pair in the phrase table. The empirical evaluation reveals that this approach of scoring phrase pairs with an RNN Encoder-Decoder improves the translation performance.We qualitatively analyze the trained RNN Encoder-Decoder by comparing its phrase scores with those given by the existing translation model. The qualitative analysis shows that the RNN Encoder-Decoder is better at capturing the linguistic regularities in the phrase table, indirectly explaining the quantitative improvements in the overall translation performance. The further analysis of the model reveals that the RNN Encoder-Decoder learns a continuous space representation of a phrase that preserves both the semantic and syntactic structure of the phrase. A recurrent neural network (RNN) is a neural network that consists of a hidden state hand an optional output y which operates on a variablelength sequence x = (x 1 , . . . , x T ). At each time step t, the hidden state ht of the RNN is updated bywhere f is a non-linear activation function. f maybe as simple as an elementwise logistic sigmoid function and as complex as along short-term memory (LSTM) unit).An RNN can learn a probability distribution over a sequence by being trained to predict the next symbol in a sequence. In that case, the output at each timestep t is the conditional distribution p(x t | x t?1 , . . . , x 1 ). For example, a multinomial distribution (1-of-K coding) can be output using a softmax activation functionfor all possible symbols j = 1, . . . , K, where w j are the rows of a weight matrix W. By combining these probabilities, we can compute the probability of the sequence x usingFrom this learned distribution, it is straightforward to sample anew sequence by iteratively sampling a symbol at each time step.\n",
      "\n",
      "\n",
      "In neural language modelling, a neural network estimates a distribution over sequences of words or characters that belong to a given language. In neural machine translation, the network estimates a distribution over sequences in the target language conditioned on a given sequence in the source language. The network can bethought of as composed of two parts: a source network (the encoder) that encodes the source sequence into a representation and a target network (the decoder) that uses the representation of the source encoder to generate the target sequence.Recurrent neural networks (RNN) are powerful sequence models and are widely used in language modelling), yet they have a potential drawback. RNNs have an inherently serial structure that prevents them from being run in parallel along the sequence length during training and evaluation. Forward and backward signals in a RNN also need to traverse the full distance of the serial path to reach from one token in the sequence to another. The larger the distance, the harder it is to learn the dependencies between the tokens.A number of neural architectures have been proposed for modelling translation, such as encoder-decoder networks, networks with attentional pooling and twodimensional networks. Despite the generally good performance, the proposed models arXiv:1610.10099v2 [cs.CL] 15 Mar 2017 EOS EOS EOS |s| |t| |t|. Dynamic unfolding in the ByteNet architecture. At each step the decoder is conditioned on the source representation produced by the encoder for that step, or simply on no representation for steps beyond the extended length |t|. The decoding ends when the target network produces an end-of-sequence (EOS) symbol. either have running time that is super-linear in the length of the source and target sequences, or they process the source sequence into a constant size representation, burdening the model with a memorization step. Both of these drawbacks grow more severe as the length of the sequences increases.We present a family of encoder-decoder neural networks that are characterized by two architectural mechanisms aimed to address the drawbacks of the conventional approaches mentioned above. The first mechanism involves the stacking of the decoder on top of the representation of the encoder in a manner that preserves the temporal resolution of the sequences; this is in contrast with architectures that encode the source into a fixed-size representation. The second mechanism is the dynamic unfolding mechanism that allows the network to process in a simple and efficient way source and target sequences of different lengths (Sect. 3.2).The ByteNet is the instance within this family of models that uses one-dimensional convolutional neural networks (CNN) of fixed depth for both the encoder and the decoder). The two CNNs use increasing factors of dilation to rapidly grow the receptive fields; a similar technique is also used in. The convolutions in the decoder CNN are masked to prevent the network from seeing future tokens in the target sequence .The network has beneficial computational and learning properties. From a computational perspective, the network has a running time that is linear in the length of the source and target sequences (up to a constant c ? log d where dis the size of the desired dependency field). The computation in the encoder during training and decoding and in the decoder during training can also be run efficiently in parallel along the sequences (Sect. 2). From a learning perspective, the representation of the source sequence in the ByteNet is resolution preserving; the representation sidesteps the need for memorization and allows for maximal bandwidth between encoder and decoder. In addition, the distance traversed by forward and backward signals between any input and output tokens corresponds to the fixed depth of the networks and is largely independent of the dis-tance between the tokens. Dependencies overlarge distances are connected by short paths and can be learnt more easily.We apply the ByteNet model to strings of characters for character-level language modelling and character-tocharacter machine translation. We evaluate the decoder network on the Hutter Prize Wikipedia task where it achieves the state-of-the-art performance of 1.31 bits/character. We further evaluate the encoderdecoder network on character-to-character machine translation on the English-to-German WMT benchmark where it achieves a state-of-the-art BLEU score of 22.85 (0.380 bits/character) and 25.53 (0.389 bits/character) on the 2014 and 2015 test sets, respectively. On the character-level machine translation task, ByteNet betters a comparable version of GNMT that is a state-of-the-art system. These results show that deep CNNs are simple, scalable and effective architectures for challenging linguistic processing tasks.The paper is organized as follows. Section 2 lays out the background and some desiderata for neural architectures underlying translation models. Section 3 defines the proposed family of architectures and the specific convolutional instance (ByteNet) used in the experiments. Section 4 analyses ByteNet as well as existing neural translation models based on the desiderata set out in Section 2. Section 5 reports the experiments on language modelling and Section 6 reports the experiments on character-to-character machine translation.\n",
      "\n",
      "\n",
      "Recurrent neural networks, long short-term memory and gated recurrent neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures.Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states ht , as a function of the previous hidden state h t?1 and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved significant improvements in computational efficiency through factorization tricks and conditional computation, while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains.Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences. In all but a few cases, however, such attention mechanisms are used in conjunction with a recurrent network.In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach anew state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n",
      "\n",
      "\n",
      "Neural machine translation (NMT) has attracted a lot of interest in solving the machine translation (MT) problem in recent years. Unlike conventional statistical machine translation (SMT) systems which consist of multiple separately tuned components, NMT models encode the source sequence into continuous representation space and generate the target sequence in an end-to-end fashon. Moreover, NMT models can also be easily adapted to other tasks such as dialog systems , question answering systems and image caption generation.In general, there are two types of NMT topologies: the encoder-decoder network and the attention network. The encoder-decoder network represents the source sequence with a fixed dimensional vector and the target sequence is generated from this vector word byword. The attention network uses the representations from all time steps of the input sequence to build a detailed relationship between the target words and the input words. Recent results show that the systems based on these models can achieve similar performance to conventional SMT systems.However, a single neural model of either of the above types has not been competitive with the best conventional system when evaluated on the WMT'14 English-to-French task. The best BLEU score from a single model with six layers is only 31.5 while the conventional method of achieves 37.0.We focus on improving the single model perfor-mance by increasing the model depth. Deep topology has been proven to outperform the shallow architecture in computer vision. In the past two years the top positions of the ImageNet contest have always been occupied by systems with tensor even hundreds of layers. But in NMT, the biggest depth used successfully is only six. We attribute this problem to the properties of the Long Short-Term Memory (LSTM) which is widely used in NMT. In the LSTM, there are more non-linear activations than in convolution layers. These activations significantly decrease the magnitude of the gradient in the deep topology, especially when the gradient propagates in recurrent form. There are also many efforts to increase the depth of the LSTM such as the work by, where the shortcuts do not avoid the nonlinear and recurrent computation.In this work, we introduce anew type of linear connections for multi-layer recurrent networks. These connections, which are called fast-forward connections, play an essential role in building a deep topology with depth of 16. In addition, we introduce an interleaved bi-directional architecture to stack LSTM layers in the encoder. This topology can be used for both the encoder-decoder network and the attention network. On the WMT'14 Englishto-French task, this is the deepest NMT topology that has ever been investigated. With our deep attention model, the BLEU score can be improved to 37.7 outperforming the shallow model which has six layers by 6.2 BLEU points. This is also the first time on this task that a single NMT model achieves state-of-the-art performance and outperforms the best conventional SMT system with an improvement of 0.7. Even without using the attention mechanism, we can still achieve 36.3 with a single model. After model ensembling and unknown word processing, the BLEU score can be further improved to 40.4. When evaluated on the subset of the test corpus without unknown words, our model achieves 41.4. As a reference, previous work showed that oracle rescoring of the 1000-best sequences generated by the SMT model can achieve the BLEU score of about 45. Our models are also validated on the more difficult WMT'14 English-to-German task.\n",
      "\n",
      "\n",
      "Neural machine translation, directly applying a single neural network to transform the source sentence into the target sentence, has now reached impressive performance. The NMT typically consists of two sub neural networks. The encoder network reads and encodes the source sentence into a 1 Feng Wang is the corresponding author of this paper context vector, and the decoder network generates the target sentence iteratively based on the context vector. NMT can be studied in supervised and unsupervised learning settings. In the supervised setting, bilingual corpora is available for training the NMT model. In the unsupervised setting, we only have two independent monolingual corpora with one for each language and there is no bilingual training example to provide alignment information for the two languages. Due to lack of alignment information, the unsupervised NMT is considered more challenging. However, this task is very promising, since the monolingual corpora is usually easy to be collected.Motivated by recent success in unsupervised cross-lingual embeddings, the models proposed for unsupervised NMT often assume that a pair of sentences from two different languages can be mapped to a same latent representation in a shared-latent space. Following this assumption, use a single encoder and a single decoder for both the source and target languages. The encoder and decoder, acting as a standard auto-encoder (AE), are trained to reconstruct the inputs. And utilize a shared encoder but two independent decoders. With some good performance, they share a glaring defect, i.e., only one encoder is shared by the source and target languages. Although the shared encoder is vital for mapping sentences from different languages into the shared-latent space, it is weak in keeping the uniqueness and internal characteristics of each language, such as the style, terminology and sentence structure. Since each language has its own characteristics, the source and target languages should be encoded and learned independently. Therefore, we conjecture that the shared encoder maybe a factor limit-ing the potential translation performance.In order to address this issue, we extend the encoder-shared model, i.e., the model with one shared encoder, by leveraging two independent encoders with each for one language. Similarly, two independent decoders are utilized. For each language, the encoder and its corresponding decoder perform an AE, where the encoder generates the latent representations from the perturbed input sentences and the decoder reconstructs the sentences from the latent representations. To map the latent representations from different languages to a shared-latent space, we propose the weightsharing constraint to the two AEs. Specifically, we share the weights of the last few layers of two encoders that are responsible for extracting highlevel representations of input sentences. Similarly, we share the weights of the first few layers of two decoders. To enforce the shared-latent space, the word embeddings are used as a reinforced encoding component in our encoders. For cross-language translation, we utilize the backtranslation following. Additionally, two different generative adversarial networks (GAN) , namely the local and global GAN, are proposed to further improve the cross-language translation. We utilize the local GAN to constrain the source and target latent representations to have the same distribution, whereby the encoder tries to fool a local discriminator which is simultaneously trained to distinguish the language of a given latent representation. We apply the global GAN to finetune the corresponding generator, i.e., the composition of the encoder and decoder of the other language, where a global discriminator is leveraged to guide the training of the generator by assessing how far the generated sentence is from the true data distribution 1 . In summary, we mainly make the following contributions:• We propose the weight-sharing constraint to unsupervised NMT, enabling the model to utilize an independent encoder for each language. To enforce the shared-latent space, we also propose the embedding-reinforced encoders and two different GANs for our model.• We conduct extensive experiments on The code that we utilized to train and evaluate our models can be found at https://github.com/ZhenYangIACAS/unsupervised-NMT English-German, English-French and Chinese-to-English translation tasks. Experimental results show that the proposed approach consistently achieves great success.• Last but not least, we introduce the directional self-attention to model temporal order information for the proposed model. Experimental results reveal that it deserves more efforts for researchers to investigate the temporal order information within self-attention layers of NMT.\n",
      "\n",
      "\n",
      "Neural machine translation (NMT) is a rapidly changing research area. Since 2016 when NMT systems first showed to achieve significantly better results than statistical machine translation (SMT) systems, the dominant neural network (NN) architectures for NMT have changed on a yearly (and even more frequent) basis. The state-of-the-art in 2016 were shallow attention-based recurrent neural networks (RNN) with gated recurrent units (GRU)  in recurrent layers. In 2017, multiplicative long short-term memory (MLSTM) units and deep GRU models were introduced in NMT. The same year, selfattentional (Transformer) models were introduced. Consequently, in 2018, most of the top scoring systems in the shared task on news translation of the Third Conference on Machine Translation (WMT) were trained using Transformer models 1 . However, it is already evident that the state-of-the-art architectures will 1 All 14 of the best automatically scored systems according to the information provided by participants in the official submission portal http://matrix.statmt.org were indicated as being based on Transformer models. be pushed even further in 2018. For instance, have recently proposed RNMT+ models that combine deep LSTM-based models with multi-head attention and showed that the models outperform Transformer models.In WMT 2017, Tilde participated with MLSTM-based NMT systems. In this paper, we compare the MLSTMbased models with Transformer models for English-Estonian and Estonian-English and we show that the state-of-the-art of WMT 2017 is well behind the new models. Therefore, for WMT 2018, Tilde submitted NMT systems that were trained using Transformer models.The paper is further structured as follows: Section 2 provides an overview of systems submitted for the WMT 2018 shared task on news translation, Section 3 describes the data used to train the NMT systems and the data pre-processing workflows, Section 4 describes all NMT systems trained and experiments on handling of named entities and combination of systems, Section 5 provides automatic evaluation results, and Section 6 concludes the paper.\n",
      "\n",
      "\n",
      "Word embeddings, which are distributed and continuous vector representations for word tokens, have been one of the basic building blocks for many neural network-based models used in natural language processing (NLP) tasks, such as language modeling, text classification and machine translation. Different from classic one-hot representation, the learned word embeddings contain semantic information which can measure the semantic similarity between words, and can also be transferred into other learning tasks.In deep learning approaches for NLP tasks, word embeddings act as the inputs of the neural network and are usually trained together with neural network parameters. As the inputs of the neural network, word embeddings carryall the information of words that will be further processed by the network, and the quality of embeddings is critical and highly impacts the final performance of the learning task. Unfortunately, we find the word embeddings learned by many deep learning approaches are far from perfect. As shown in(a) and 1(b), in the embedding space learned by word2vec model, the nearest neighbors of word \"Peking\" includes \"quickest\", \"multicellular\", and \"epigenetic\", which are not semantically similar, while semantically related words such as \"Beijing\" and \"China\" are far from it. Similar phenomena are observed from the word embeddings learned from translation tasks.With a careful study, we find a more general problem which is rooted in low-frequency words in the text corpus. Without any confusion, we also call high-frequency words as popular words and call low-frequency words as rare words. As is well known, the frequency distribution of words roughly follows a simple mathematical form known as Zipf's law. When the size of a text corpus grows, the frequency of rare words is much smaller than popular words while the number of unique rare words is much larger than popular words. Interestingly, the learned embeddings of rare words and popular words behave differently. In the embedding space, a popular word usually has semantically related neighbors, while a rare word usually does not. Moreover, the nearest neighbors of more than 85% rare words are rare words. Word embeddings encode frequency information. As shown in(a) and 1(b), the embeddings of rare words and popular words actually lie in different subregions of the space. Such a phenomenon is also observed in.We argue that the different behaviors of the embeddings of popular words and rare words are problematic. First, such embeddings will affect the semantic understanding of words. We observe more than half of the rare words are nouns or variants of popular words. Those rare words should have similar meanings or share the same topics with popular words. Second, the neighbors of a large number of rare words are semantically unrelated rare words. To some extent, those word embeddings encode more frequency information than semantic information which is not good from the view of semantic understanding. It will consequently limit the performance of down-stream tasks using the embeddings. For example, in text classification, it cannot be well guaranteed that the label of a sentence does not change when you replace one popular/rare word in the sentence by its rare/popular alternatives.To address this problem, in this paper, we propose an adversarial training method to learn FRequency-AGnostic word Embedding (FRAGE). For a given NLP task, in addition to minimize the task-specific loss by optimizing the task-specific parameters together with word embeddings, we introduce another discriminator, which takes a word embedding as input and classifies whether it is a popular/rare word. The discriminator optimizes its parameters to maximize its classification accuracy, while word embeddings are optimized towards a low task-dependent loss as well as fooling the discriminator to mis-classify the popular and rare words. When the whole training process converges and the system achieves an equilibrium, the discriminator cannot well differentiate popular words from rare words. Consequently, rare words lie in the same region as and are mixed with popular words in the embedding space. Then FRAGE will catch better semantic information and help the task-specific model to perform better.We conduct experiments on four types of NLP tasks, including three word similarity tasks, two language modeling tasks, three sentiment classification tasks and two machine translation tasks to test our method. In all tasks, FRAGE outperforms the baselines. Specifically, in language modeling and machine translation, we achieve better performance than the state-of-the-art results on PTB, WT2 and WMT14 English-German datasets.\n",
      "\n",
      "\n",
      "Due to the explosive growth of data, subset selection methods are increasingly popular fora wide range of machine learning and computer vision applications. This kind of methods offer the potential to select a few highly representative samples or exemplars to describe the entire dataset. By analyzing a few, we can roughly know all. Such case is very important to summarize and visualize huge datasets of texts, images and videos etc.. Besides, by only using the selected exemplars for succeeding tasks, the cost of memories and computational time will be greatly reduced. Additionally, as outliers are generally less representative, the side effect of outliers will be reduced, thus boosting the performance of subsequent applications.There have been several subset selection methods. The most intuitional method is to randomly select a fixed number of samples. Although highly efficient, there is no guarantee for an effective selection. For the other methods, depending on the mechanism of representative exemplars, there are mainly three categories of selection methods. One category Data size (N ) Selection Time\n",
      "\n",
      "\n",
      "Named entity recognition (NER) is a challenging learning problem. One the one hand, inmost languages and domains, there is only a very small amount of supervised training data available. On the other, there are few constraints on the kinds of words that can be names, so generalizing from this small sample of data is difficult. As a result, carefully constructed orthographic features and language-specific knowledge resources, such as gazetteers, are widely used for solving this task. Unfortunately, languagespecific resources and features are costly to develop in new languages and new domains, making NER a challenge to adapt. Unsupervised learning from unannotated corpora offers an alternative strategy for obtaining better generalization from small amounts of supervision. However, even systems that have relied extensively on unsupervised features have used these to augment, rather than replace, hand-engineered features (e.g., knowledge about capitalization patterns and character classes in a particular language) and specialized knowledge resources (e.g., gazetteers).In this paper, we present neural architectures for NER that use no language-specific resources or features beyond a small amount of supervised training data and unlabeled corpora. Our models are designed to capture two intuitions. First, since names often consist of multiple tokens, reasoning jointly over tagging decisions for each token is important. We compare two models here, (i) a bidirectional LSTM with a sequential conditional random layer above it (LSTM-CRF; §2), and (ii) anew model that constructs and labels chunks of input sentences using an algorithm inspired by transition-based parsing with states represented by stack LSTMs (S-LSTM; §3). Second, token-level evidence for \"being a name\" includes both orthographic evidence (what does the word being tagged as a name look like?) and distributional evidence (where does the word being tagged tend to occur in a corpus?). To capture orthographic sensitivity, we use character-based word representation model to capture distributional sensitivity, we combine these representations with distributional representations. Our word representations combine both of these, and dropout training is used to encourage the model to learn to trust both sources of evidence ( §4).Experiments in English, Dutch, German, and Spanish show that we are able to obtain state-of-the-art NER performance with the LSTM-CRF model in Dutch, German, and Spanish, and very near the state-of-the-art in English without any hand-engineered features or gazetteers ( §5). The transition-based algorithm likewise surpasses the best previously published results in several languages, although it performs less well than the LSTM-CRF model.\n",
      "\n",
      "\n",
      "In order to democratize large-scale NLP and information extraction while minimizing our environmental footprint, we require fast, resource-efficient methods for sequence tagging tasks such as part-of-speech tagging and named entity recognition (NER). Speed is not sufficient of course: they must also be expressive enough to tolerate the tremendous lexical variation in input data.The massively parallel computation facilitated by GPU hardware has led to a surge of successful neural network architectures for sequence labeling. While these models are expressive and accurate, they fail to fully exploit the parallelism opportunities of a GPU, and thus their speed is limited. Specifically, they employ either recurrent neural networks (RNNs) for feature extraction, or Viterbi inference in a structured output model, both of which require sequential computation across the length of the input.Instead, parallelized runtime independent of the length of the sequence saves time and energy costs, maximizing GPU resource usage and minimizing the amount of time it takes to train and evaluate models. Convolutional neural networks (CNNs) provide exactly this property. Rather than composing representations incrementally over each token in a sequence, they apply filters in parallel across the entire sequence at once. Their computational cost grows with the number of layers, but not the input size, up to the memory and threading limitations of the hardware. This provides, for example, audio generation models that can be trained in parallel (van den.Despite the clear computational advantages of CNNs, RNNs have become the standard method for composing deep representations of text. This is because a token encoded by a bidirectional RNN will incorporate evidence from the entire input sequence, but the CNN's representation is limited by the effective input width 1 of the network: the size of the input context which is observed, directly or indirectly, by the representation of a token at a given layer in the network. Specifically, in a network composed of a series of stacked convolutional layers of convolution width w, the number r of context tokens incorporated into a token's representation at a given layer l, is given by r = l(w ? 1) + 1. The number of layers required to incorporate the entire input context grows linearly with the length of the sequence. To avoid this scaling, one could pool representations across the sequence, but this is not appropriate for sequence labeling, since it reduces the output resolution of the representation.In response, this paper presents an application of dilated convolutions for sequence labeling). For dilated convolutions, the effective input width can grow exponentially with the depth, with no loss in resolution at each layer and with a modest number of parameters to estimate. Like typical CNN layers, dilated convolutions operate on a sliding window of context over the sequence, but unlike conventional convolutions, the context need not be consecutive; the dilated window skips over every dilation width d inputs. By stacking layers of dilated convolutions of exponentially increasing dilation width, we can expand the size of the effective input width to cover the entire length of most sequences using only a few layers: The size of the effective input width fora token at layer l is now given by 2 l+1 ?1. More concretely, just four stacked dilated convolutions of width 3 produces token representations with an effective input width of 31 tokens -longer than the average sentence length (23) in the Penn TreeBank.Our overall iterated dilated CNN architecture (ID-CNN) repeatedly applies the same block of dilated convolutions to token-wise representations. This parameter sharing prevents overfitting and also provides opportunities to inject supervision on intermediate activations of the network. Similar to models that use logits produced by an RNN, the ID-CNN provides two methods for performing prediction: we can predict each token's label independently, or by running Viterbi inference in a chain structured graphical model.In experiments on CoNLL 2003 and OntoNotes 1 What we call effective input width here is known as the receptive field in the vision literature, drawing an analogy to the visual receptive field of a neuron in the retina.: A dilated CNN block with maximum dilation width 4 and filter width 3. Neurons contributing to a single highlighted neuron in the last layer are also highlighted. 5.0 English NER, we demonstrate significant speed gains of our ID-CNNs over various recurrent models, while maintaining similar F1 performance. When performing prediction using independent classification, the ID-CNN consistently outperforms a bidirectional LSTM (Bi-LSTM), and performs on par with inference in a CRF with logits from a Bi-LSTM (Bi-LSTM-CRF). As an extractor of per-token logits fora CRF, our model out-performs the Bi-LSTM-CRF. We also apply ID-CNNs to entire documents, where independent token classification is as accurate as the Bi-LSTM-CRF while decoding almost 8× faster. The clear accuracy gains resulting from incorporating broader context suggest that these models could similarly benefit many other contextsensitive NLP tasks which have until now been limited by the computational complexity of existing context-rich models. 2 2 Background\n",
      "\n",
      "\n",
      "Due to their simplicity and efficacy, pre-trained word embedding have become ubiquitous in NLP systems. Many prior studies have shown that they capture useful semantic and syntactic information and including them in NLP systems has been shown to be enormously helpful fora variety of downstream tasks.However, in many NLP tasks it is essential to represent not just the meaning of a word, but also the word in context. For example, in the two phrases \"A Central Bank spokesman\" and \"The Central African Republic\", the word 'Central' is used as part of both an Organization and Location. Accordingly, current state of the art sequence tagging models typically include a bidirectional re-current neural network (RNN) that encodes token sequences into a context sensitive representation before making token specific predictions.Although the token representation is initialized with pre-trained embeddings, the parameters of the bidirectional RNN are typically learned only on labeled data. Previous work has explored methods for jointly learning the bidirectional RNN with supplemental labeled data from other tasks (e.g.,.In this paper, we explore an alternate semisupervised approach which does not require additional labeled data. We use a neural language model (LM), pre-trained on a large, unlabeled corpus to compute an encoding of the context at each position in the sequence (hereafter an LM embedding) and use it in the supervised sequence tagging model. Since the LM embeddings are used to compute the probability of future words in a neural LM, they are likely to encode both the semantic and syntactic roles of words in context.Our main contribution is to show that the context sensitive representation captured in the LM embeddings is useful in the supervised sequence tagging setting. When we include the LM embeddings in our system overall performance increases from 90.87% to 91.93% F 1 for the CoNLL 2003 NER task, a more then 1% absolute F1 increase, and a substantial improvement over the previous state of the art. We also establish anew state of the art result (96.37% F 1 ) for the CoNLL 2000 Chunking task.As a secondary contribution, we show that using both forward and backward LM embeddings boosts performance over a forward only LM. We also demonstrate that domain specific pre-training is not necessary by applying a LM trained in the news domain to scientific papers. The main components in our language-modelaugmented sequence tagger (TagLM) are illustrated in. After pre-training word embeddings and a neural LM on large, unlabeled corpora (Step 1), we extract the word and LM embeddings for every token in a given input sequenceStep 2) and use them in the supervised sequence tagging model (Step 3).\n",
      "\n",
      "\n",
      "Pre-trained word representations area key component in many neural language understanding models. However, learning high quality representations can be challenging. They should ideally model both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). In this paper, we introduce anew type of deep contextualized word representation that directly addresses both challenges, can be easily integrated into existing models, and significantly improves the state of the art in every considered case across a range of challenging language understanding problems.Our representations differ from traditional word type embeddings in that each token is assigned a representation that is a function of the entire input sentence. We use vectors derived from a bidirectional LSTM that is trained with a coupled lan-guage model (LM) objective on a large text corpus. For this reason, we call them ELMo (Embeddings from Language Models) representations. Unlike previous approaches for learning contextualized word vectors, ELMo representations are deep, in the sense that they area function of all of the internal layers of the biLM. More specifically, we learn a linear combination of the vectors stacked above each input word for each end task, which markedly improves performance over just using the top LSTM layer.Combining the internal states in this manner allows for very rich word representations. Using intrinsic evaluations, we show that the higher-level LSTM states capture context-dependent aspects of word meaning (e.g., they can be used without modification to perform well on supervised word sense disambiguation tasks) while lowerlevel states model aspects of syntax (e.g., they can be used to do part-of-speech tagging). Simultaneously exposing all of these signals is highly beneficial, allowing the learned models select the types of semi-supervision that are most useful for each end task.Extensive experiments demonstrate that ELMo representations work extremely well in practice. We first show that they can be easily added to existing models for six diverse and challenging language understanding problems, including textual entailment, question answering and sentiment analysis. The addition of ELMo representations alone significantly improves the state of the art in every case, including up to 20% relative error reductions. For tasks where direct comparisons are possible, ELMo outperforms CoVe, which computes contextualized representations using a neural machine translation encoder. Finally, an analysis of both ELMo and CoVe reveals that deep representations outperform arXiv:1802.05365v2 [cs.CL] 22 Mar 2018 those derived from just the top layer of an LSTM. Our trained models and code are publicly available, and we expect that ELMo will provide similar gains for many other NLP problems. 1 2 Related work Due to their ability to capture syntactic and semantic information of words from large scale unlabeled text, pretrained word vectors area standard component of most state-ofthe-art NLP architectures, including for question answering, textual entailment and semantic role labeling. However, these approaches for learning word vectors only allow a single contextindependent representation for each word.Previously proposed methods overcome some of the shortcomings of traditional word vectors by either enriching them with subword information (e.g., or learning separate vectors for each word sense (e.g.,. Our approach also benefits from subword units through the use of character convolutions, and we seamlessly incorporate multi-sense information into downstream tasks without explicitly training to predict predefined sense classes.Other recent work has also focused on learning context-dependent representations. context2vec (Melamud et al., 2016) uses a bidirectional Long Short Term Memory (LSTM; Hochreiter and Schmidhuber, 1997) to encode the context around a pivot word. Other approaches for learning contextual embeddings include the pivot word itself in the representation and are computed with the encoder of either a supervised neural machine translation (MT) system or an unsupervised language model. Both of these approaches benefit from large datasets, although the MT approach is limited by the size of parallel corpora. In this paper, we take full advantage of access to plentiful monolingual data, and train our biLM on a corpus with approximately 30 million sentences. We also generalize these approaches to deep contextual representations, which we show work well across abroad range of diverse NLP tasks. 1 http://allennlp.org/elmo Previous work has also shown that different layers of deep biRNNs encode different types of information. For example, introducing multi-task syntactic supervision (e.g., part-of-speech tags) at the lower levels of a deep LSTM can improve overall performance of higher level tasks such as dependency parsing or CCG super tagging. In an RNN-based encoder-decoder machine translation system, showed that the representations learned at the first layer in a 2layer LSTM encoder are better at predicting POS tags then second layer. Finally, the top layer of an LSTM for encoding word context has been shown to learn representations of word sense. We show that similar signals are also induced by the modified language model objective of our ELMo representations, and it can be very beneficial to learn models for downstream tasks that mix these different types of semi-supervision. and pretrain encoder-decoder pairs using language models and sequence autoencoders and then fine tune with task specific supervision. In contrast, after pretraining the biLM with unlabeled data, we fix the weights and add additional taskspecific model capacity, allowing us to leverage large, rich and universal biLM representations for cases where downstream training data size dictates a smaller supervised model.\n",
      "\n",
      "\n",
      "Neural models have become the dominant approach in the NLP literature. Compared to handcrafted indicator features, neural sentence representations are less sparse, and more flexible in encoding intricate syntactic and semantic information. Among various neural networks for encoding sentences, bi-directional LSTMs (BiLSTM) have been a dominant method, giving state-of-the-art results in language modelling, machine translation, syntactic parsing and question answering.Despite their success, BiLSTMs have been shown to suffer several limitations. For example, their inherently sequential nature endows computation non-parallel within the same sentence, which can lead to a computational bottleneck, hindering their use in the in- dustry. In addition, local ngrams, which have been shown a highly useful source of contextual information for NLP, are not explicitly modelled. Finally, sequential information flow leads to relatively weaker power in capturing longrange dependencies, which results in lower performance in encoding longer sentences.We investigate an alternative recurrent neural network structure for addressing these issues. As shown in, the main idea is to model the hidden states of all words simultaneously at each recurrent step, rather than one word at a time. In particular, we view the whole sentence as a single state, which consists of sub-states for individual words and an overall sentence-level state. To capture local and non-local contexts, states are updated recurrently by exchanging information between each other. Consequently, we refer to our model as sentence-state LSTM, or S-LSTM in short. Empirically, S-LSTM can give effective sentence encoding after 3 -6 recurrent steps. In contrast, the number of recurrent steps necessary for BiLSTM scales with the size of the sentence.At each recurrent step, information exchange is conducted between consecutive words in the sentence, and between the sentence-level state and each word. In particular, each word receives information from its predecessor and successor simultaneously. From an initial state without information exchange, each word-level state can obtain 3-gram, 5-gram and 7-gram information after 1, 2 and 3 recurrent steps, respectively. Being connected with every word, the sentence-level state vector serves to exchange non-local information with each word. In addition, it can also be used as a global sentence-level representation for classification tasks.Results on both classification and sequence labelling show that S-LSTM gives better accuracies compared to BiLSTM using the same number of parameters, while being faster. We release our code and models at https://github.com/ leuchine/S-LSTM, which include all baselines and the final model.\n",
      "\n",
      "\n",
      "Named-Entity Recognition (NER) is the task of identifying textual mentions and classifying them into a predefined set of types. Various approaches have been proposed to tackle the task, from hand-crafted feature-based machine learning models like conditional random fields and perceptron, to deep neural models.Word representations, also known as word embeddings, area key element for multiple NLP tasks including NER. Due to the small amount of named-entity annotated data, embeddings are used to extend, rather than replace, hand-crafted features in order to obtain state-of-the-art performance. Recent studies have explored methods for supplying deep sequential taggers with complementary features to standard embeddings. and tested special embeddings extracted from a neural language model (LM) trained on a large corpus. LM embeddings capture context-dependent aspects of word meaning using future (forward LM) and previous (backward LM) context words. When this information is added to standard features, it leads to significant improvements in NER. Also, showed that external knowledge resources (namely gazetteers) are crucial to NER performance. Gazetteer features encode the presence of word n-grams in predefined lists of NEs.In this work, we discuss some of the limitations of gazetteer features and propose an alternative lexical representation which is trained offline and that can be added to any neural NER system. Ina nutshell, we embed words and entity types into a joint vector space by leveraging WiFiNE, a ressource which automatically annotates mentions in Wikipedia with 120 entity types. From this vector space, we compute for each word a 120-dimensional vector, where each dimension encodes the similarity of the word with an entity type. We call this vector an LS representation, for Lexical Similarity. When included in a vanilla LSTM-CRF NER model, LS representations lead to significant gains. We establish anew state-of-the-art F1 score of 87.95 on ONTONOTES 5.0, while matching state-of-the-art performance on the over-studied In the rest of this paper, we motivate our work in Section 2. We describe how we compute LS vectors in Section 3. We present our system in Section 4 and report results in Section 5. In Section 6, we discuss related works before concluding in Section 7.\n",
      "\n",
      "\n",
      "There has been an increasing interest in named entity recognition or more generally recognizing entity mentions 2) that the nested hierarchical structure of entity mentions should betaken into account to better facilitate downstream tasks like question answering, relation extraction, event extraction, and coreference resolution. Practically, the mentions with nested structures frequently exist in news and biomedical documents. For example in  Traditional sequence labeling models such as conditional random fields (CRF) do not allow hierarchical structures between segments, making them incapable to handle such problems. presented a chart-based parsing approach where each sentence with nested mentions is mapped to a rooted constituent tree. The issue of using a chart-based parser is its cubic time complexity in the number of words in the sentence.To achieve a scalable and effective solution for recognizing nested mentions, we design a transition-based system which is inspired by the recent success of employing transition-based methods for constituent parsing) and named entity recognition, especially when they are paired with neural networks. Generally, each sentence with nested mentions is mapped to a forest where each outermost mention forms a tree consisting of its inner mentions. Then our transition-based system learns to construct this forest through a sequence of shift-reduce actions. shows an example of such a forest. In contrast, the tree structure by further uses a root node to connect all tree elements. Our forest representation eliminates the root node so that the number of actions required to construct it can be reduced significantly.Following, we employ Stack-LSTM to represent the system's state, which consists of the states of input, stack and action history, in a continuous space incrementally. The (partially) processed nested mentions in the stack are encoded with recursive neural networks where composition functions are used to capture dependencies between nested mentions. Based on the observation that letter-level patterns such as capitalization and prefix can be beneficial in detecting mentions, we incorporate a characterlevel LSTM to capture such morphological information. Meanwhile, this character-level component can also help deal with the out-of-vocabulary problem of neural models. We conduct experiments in three standard datasets. Our system achieves the state-of-the-art performance on ACE datasets and comparable performance in GENIA dataset.\n",
      "\n",
      "\n",
      "Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (OpenAI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches. The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in OpenAI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers. BERT alleviates the previously mentioned unidirectionality constraint by using a \"masked language model\" (MLM) pre-training objective, inspired by the Cloze task. The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked word based only on its context. Unlike left-toright language model pre-training, the MLM objective enables the representation to fuse the left and the right context, which allows us to pretrain a deep bidirectional Transformer. In addition to the masked language model, we also use a \"next sentence prediction\" task that jointly pretrains text-pair representations. The contributions of our paper are as follows:• We demonstrate the importance of bidirectional pre-training for language representations. Unlike, which uses unidirectional language models for pre-training, BERT uses masked language models to enable pretrained deep bidirectional representations. This is also in contrast to, which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs.• We show that pre-trained representations reduce the need for many heavily-engineered taskspecific architectures. BERT is the first finetuning based representation model that achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks, outperforming many task-specific architectures.• BERT advances the state of the art for eleven NLP tasks. The code and pre-trained models are available at https://github.com/ google-research/bert.\n",
      "\n",
      "\n",
      "The volume of biomedical literature continues to rapidly increase. On average, more than 3000 new articles are published everyday in peer-reviewed journals, excluding pre-prints and technical reports such as clinical trial reports in various archives. PubMed alone has a total of 29M articles as of January 2019. Reports containing valuable information about new discoveries and new insights are continuously added to the already overwhelming amount of literature. Consequently, there is increasingly more demand for accurate biomedical text mining tools for extracting information from the literature.Recent progress of biomedical text mining models was made possible by the advancements of deep learning techniques used in natural language processing (NLP). For instance, Long Short-Term Memory (LSTM) and Conditional Random Field (CRF) have greatly improved performance in biomedical named entity recognition (NER) over the last few years. Other deep learning based models have made improvements in biomedical text mining tasks such as relation extraction (RE) and question answering (QA).However, directly applying state-of-the-art NLP methodologies to biomedical text mining has limitations. First, as recent word representation models such as Word2Vec, ELMo and BERT are trained and tested mainly on datasets containing general domain texts (e.g. Wikipedia), it is difficult to estimate their performance on datasets containing biomedical texts. Also, the word distributions of general and biomedical corpora are quite different, which can often be a problem for biomedical text mining models. As a result, recent models in biomedical text mining rely largely on adapted versions of word representations.In this study, we hypothesize that current state-of-the-art word representation models such as BERT need to be trained on biomedical corpora to be effective in biomedical text mining tasks. Previously, Word2Vec, which is one of the most widely known context independent word representation models, was trained on biomedical corpora which contain terms and expressions that are usually not included in a general domain corpus. While ELMo and BERT have proven the effectiveness of contextualized word representations, they cannot obtain high performance on biomedical corpora because they are pre-trained on only general domain corpora. As BERT achieves very strong results on various NLP tasks while using almost the same structure across the tasks, adapting BERT for the biomedical domain could potentially benefit numerous biomedical NLP researches.\n",
      "\n",
      "\n",
      "This paper addresses the challenging problem of open-domain question answering, which consists of building systems able to answer questions from any domain. Any advance on this difficult topic would bring a huge leap forward in building new ways of accessing knowledge. An important development in this area has been the creation of large-scale Knowledge Bases (KBs), such as Freebase and DBpedia which store huge amounts of general-purpose information. They are organized as databases of triples connecting pairs of entities by various relationships and of the form (left entity, relationship, right entity). Question answering is then defined as the task of retrieving the correct entity or set of entities from a KB given a query expressed as a question in natural language.The use of KBs simplifies the problem by separating the issue of collecting and organizing information (i.e. information extraction) from the one of searching through it (i.e. question answering or natural language interfacing). However, open question answering remains challenging because of the scale of these KBs (billions of triples, millions of entities and relationships) and of the difficulty for machines to interpret natural language. Recent progress has been made by tackling this problem with semantic parsers. These methods convert questions into logical forms or database queries (e.g. in SPARQL) which are then subsequently used to query KBs for answers. Even if such systems have shown the ability to handle large-scale KBs, they require practitioners to hand-craft lexicons, grammars, and KB schema for the parsing to be effective. This nonnegligible human intervention might not be generic enough to conveniently scale up to new databases with other schema, broader vocabularies or other languages than English.In this paper, we instead take the approach of converting questions to (uninterpretable) vectorial representations which require no pre-defined grammars or lexicons and can query any KB independent of its schema. Following, we focus on answering simple factual questions on abroad range of topics, more specifically, those for which single KB triples stand for both the question and an answer (of which there maybe many). For example, (parrotfish.e, live-in.r, southern-water.e) stands for What is parrotfish's habitat? and southern-water.e and (cantonese.e, be-major-language-in.r, hong-kong.e) for What is the main language of Hong-Kong? and cantonese.e. In this task, the main difficulties come from lexical variability rather than from complex syntax, having multiple answers per question, and the absence of a supervised training signal.Our approach is based on learning low-dimensional vector embeddings of words and of KB triples so that representations of questions and corresponding answers end up being similar in the embedding space. Unfortunately, we do not have access to any human labeled (query, answer) supervision for this task. In order to avoid transferring the cost of manual intervention to the one of labeling large amounts of data, we make use of weak supervision. We show empirically that our model is able to take advantage of noisy and indirect supervision by (i) automatically generating questions from KB triples and treating this as training data; and (ii) supplementing this with a data set of questions collaboratively marked as paraphrases but with no associated answers. We end up learning meaningful vectorial representations for questions involving up to 800k words and for triples of an mostly automatically created KB with 2.4M entities and 600k relationships. Our method strongly outperforms previous results on the WikiAnswers+ReVerb evaluation data set introduced by. Even if the embeddings obtained after training are of good quality, the scale of the optimization problem makes it hard to control and to lead to convergence. Thus, we propose a method to fine-tune embedding-based models by carefully optimizing a matrix parameterizing the similarity used in the embedding space, leading to a consistent improvement in performance.The rest of the paper is organized as follows. Section 2 discusses some previous work and Section 3 introduces the problem of open question answering. Then, Section 4 presents our model and Section 5 our experimental results.\n",
      "\n",
      "\n",
      "Matching two potentially heterogenous language objects is central to many natural language applications. It generalizes the conventional notion of similarity (e.g., in paraphrase identification) or relevance (e.g., in information retrieval), since it aims to model the correspondence between \"linguistic objects\" of different nature at different levels of abstractions. Examples include top-k re-ranking in machine translation (e.g., comparing the meanings of a French sentence and an English sentence) and dialogue (e.g., evaluating the appropriateness of a response to a given utterance). Natural language sentences have complicated structures, both sequential and hierarchical, that are essential for understanding them. A successful sentence-matching algorithm therefore needs to capture not only the internal structures of sentences but also the rich patterns in their interactions. Towards this end, we propose deep neural network models, which adapt the convolutional strategy (proven successful on image and speech) to natural language. To further explore the relation between representing sentences and matching them, we devise a novel model that can naturally host both the hierarchical composition for sentences and the simple-to-comprehensive fusion of matching patterns with the same convolutional architecture. Our model is generic, requiring no prior knowledge of natural language (e.g., parse tree) and putting essentially no constraints on the matching tasks. This is part of our continuing effort 1 in understanding natural language objects and the matching between them.Our main contributions can be summarized as follows. First, we devise novel deep convolutional network architectures that can naturally combine 1) the hierarchical sentence modeling through layer-by-layer composition and pooling, and 2) the capturing of the rich matching patterns at different levels of abstraction; Second, we perform extensive empirical study on tasks with different scales and characteristics, and demonstrate the superior power of the proposed architectures over competitor methods.Roadmap We start by introducing a convolution network in Section 2 as the basic architecture for sentence modeling, and how it is related to existing sentence models. Based on that, in Section 3, we propose two architectures for sentence matching, with a detailed discussion of their relation. In Section 4, we briefly discuss the learning of the proposed architectures. Then in Section 5, we report our empirical study, followed by a brief discussion of related work in Section 6.\n",
      "\n",
      "\n",
      "Open-domain Question Answering (QA) systems aim at providing the exact answer(s) to questions formulated in natural language, without restriction of domain. While there is along history of QA systems that search for textual documents or on the Web and extract answers from them (see e.g.), recent progress has been made with the release of large Knowledge Bases (KBs) such as Freebase, which contain consolidated knowledge stored as atomic facts, and extracted from different sources, such as free text, tables in webpages or collaborative input. Existing approaches for QA from KBs use learnable components to either transform the question into a structured KB query or learn to embed questions and facts in a low dimensional vector space and retrieve the answer by computing similarities in this embedding space. However, while most recent efforts have focused on designing systems with higher reasoning capabilities, that could jointly retrieve and use multiple facts to answer, the simpler problem of answering questions that refer to a single fact of the KB, which we call Simple Question Answering in this paper, is still far from solved.Hence, existing benchmarks are small; they mostly cover the head of the distributions of facts, and are restricted in their question types and their syntactic and lexical variations. As such, it is still unknown how much the existing systems perform outside the range of the specific question templates of a few, small benchmark datasets, and it is also unknown whether learning on a single dataset transfers well on other ones, and whether such systems can learn from different training sources, which we believe is necessary to capture the whole range of possible questions.Besides, the actual need for reasoning, i.e. constructing the answer from more than a single fact from the KB, depends on the actual structure of the KB. As we shall see, for instance, a simple preprocessing of Freebase tremendously increases the coverage of simple QA in terms of possible questions that can be answered with a single fact, including list questions that expect more than a single answer. In fact, the task of simple QA itself might already cover a wide range of practical usages, if the KB is properly organized. This paper presents two contributions. First, as an effort to study the coverage of existing systems and the possibility to train jointly on different data sources via multitasking, we collected the first large-scale dataset of questions and answers based on a KB, called SimpleQuestions. This dataset, which is presented in Section 2, contains more than 100k questions written by human anno-What American cartoonist is the creator of Andy Lippincott? Which forest is Fires Creek in? What is an active ingredient in childrens earache relief ?  tators and associated to Freebase facts, while the largest existing benchmark, WebQuestions, contains less than 6k questions created automatically using the Google suggest API.Second, in sections 3 and 4, we present an embedding-based QA system developed under the framework of Memory Networks (MemNNs). Memory Networks are learning systems centered around a memory component that can be read and written to, with a particular focus on cases where the relationship between the input and response languages (here natural language) and the storage language (here, the facts from KBs) is performed by embedding all of them in the same vector space. The setting of the simple QA corresponds to the elementary operation of performing a single lookup in the memory. While our model bares similarity with previous embedding models for QA, using the framework of MemNNs opens the perspective to more involved inference schemes in future work, since MemNNs were shown to perform well on complex reasoning toy QA tasks. We discuss related work in Section 5.We report experimental results in Section 6, where we show that our model achieves excellent results on the benchmark WebQuestions. We also show that it can learn from two different QA datasets to improve its performance on both. We also present the first successful application of transfer learning for QA. Using the Reverb KB and QA datasets, we show that Reverb facts can be added to the memory and used to answer without retraining, and that MemNNs achieve better results than some systems designed on this dataset.\n",
      "\n",
      "\n",
      "Humans learn in a variety of ways-by communication with each other, and by study, the reading of text. Comprehension of unstructured text by machines, at a near-human level, is a major goal for natural language processing. It has garnered significant attention from the machine learning research community in recent years.Machine comprehension (MC) is evaluated by posing a set of questions based on a text passage (akin to the reading tests we all took in school). Such tests are objectively gradable and can be used to assess a range of abilities, from basic understanding to causal reasoning to inference. Given a text passage and a question about its content, a system is tested on its ability to determine the correct answer. In this work, we focus on MCTest, a complex but data-limited comprehension benchmark, whose multiple-choice questions require not only extraction but also inference and limited reasoning. Inference and reasoning are important human skills that apply broadly, beyond language.We present a parallel-hierarchical approach to machine comprehension designed to work well in a data-limited setting. There are many use-cases in which comprehension over limited data would be handy: for example, user manuals, internal documentation, legal contracts, and soon. Moreover, work towards more efficient learning from any quantity of data is important in its own right, for bringing machines more inline with the way humans learn. Typically, artificial neural networks require numerous parameters to capture complex patterns, and the more parameters, the more training data is required to tune them. Likewise, deep models learn to extract their own features, but this is a data-intensive process. Our model learns to comprehend at a high level even when data is sparse.The key to our model is that it compares the question and answer candidates to the text using several distinct perspectives. We refer to a question combined with one of its answer candidates as a hypothesis (to be detailed below). The semantic perspective compares the hypothesis to sentences in the text viewed as single, self-contained thoughts; these are represented using a sum and transformation of word embedding vectors, similarly to in . The word-by-word perspective focuses on similarity matches between individual words from hypothesis and text, at various scales. As in the semantic perspective, we consider matches over complete sentences. We also use a sliding window acting on a subsentential scale (inspired by the work of), which implicitly considers the linear distance between matched words. Finally, this word-level sliding window operates on two different views of text sentences: the sequential view, where words appear in their natural order, and the dependency view, where words are reordered based on a linearization of the sentence's dependency graph. Words are represented throughout by embedding vectors. These distinct perspectives naturally form a hierarchy that we depict in. Language is hierarchical, so it makes sense that comprehension relies on hierarchical levels of understanding.The perspectives of our model can be considered a type of feature. However, they are implemented by parametric differentiable functions. This is in contrast to most previous efforts on MCTest, whose numerous hand-engineered features cannot be trained. Our model, significantly, can be trained end-to-end with backpropagation. To facilitate learning with limited data, we also develop a unique training scheme. We initialize the model's neural networks to perform specific heuristic functions that yield decent (thought not impressive) performance on the dataset. Thus, the training scheme gives the model a safe, reasonable baseline from which to start learning. We call this technique training wheels.Computational models that comprehend (insofar as they perform well on MC datasets) have developed contemporaneously in several research groups. Models designed specifically for MCTest include those of, and more recently,, and. In experiments, our Parallel-Hierarchical model achieves state-of-the-art accuracy on MCTest, outperforming these existing methods.Below we describe related work, the mathematical details of our model, and our experiments, then analyze our results.\n",
      "\n",
      "\n",
      "Recently, the idea of training machine comprehension models that can read, understand, and answer questions about a text has come closer to reality principally through two factors. The first is the advent of deep learning techniques, which allow manipulation of natural language beyond its surface forms and generalize beyond relatively small amounts of labeled data. The second factor is the formulation of standard machine comprehension benchmarks based on Cloze-style queries, which permit fast integration loops between model conception and experimental evaluation.Cloze-style queries are created by deleting a particular word in a natural-language statement. The task is to guess which word was deleted. Ina pragmatic approach, recent work formed such questions by extracting a sentence from a larger document. In contrast to considering a stand-alone statement, the system is now required to handle a larger amount of information that may possibly influence the prediction of the missing word. Such contextual dependencies may also be injected by removing a word from a short human-crafted summary of a larger body of text. The abstractive nature of the summary is likely to demand a higher level of comprehension of the original text. In both cases, the machine comprehension system is presented with an ablated query and the document to which the original query refers. The missing word is assumed to appear in the document. Encouraged by the recent success of deep learning attention architectures, we propose a novel neural attention-based inference model designed to perform machine reading comprehension tasks. The model first reads the document and the query using a recurrent neural network. Then, it deploys an iterative inference process to uncover the inferential links that exist between the missing query word, the query, and the document. This phase involves a novel alternating attention mechanism; it first attends to some parts of the query, then finds their corresponding matches by attending to the document. The result of this alternating search is fed back into the iterative inference process to seed the next search step. This permits our model to reason about different parts of the query in a sequential way, based on the information that has been gathered previously from the document. After a fixed number of iterations, the model uses a summary of its inference process to predict the answer. This paper makes the following contributions. We present a novel iterative, alternating attention mechanism that, unlike existing models, does not compress the query to a single representation, but instead alternates its attention between the query and the document to obtain a fine-grained query representation within a fixed computation time. Our architecture tightly integrates previous ideas related to bidirectional readers and iterative attention processes. It obtains state-of-theart results on two machine comprehension datasets and shows promise for application to abroad range of natural language processing tasks.\n",
      "\n",
      "\n",
      "Extracting semantic relations between entities in text is an important and well-studied task in information extraction and natural language processing (NLP). Traditional systems treat this task as a pipeline of two separated tasks, i.e., named entity recognition (NER)) and relation extraction, but recent studies show that end-to-end (joint) modeling of entity and relation is important for high performance since relations interact closely with entity information. For instance, to learn that Toefting and Bolton have an Organization-Affiliation (ORG-AFF) relation in the sentence Toefting transferred to Bolton, the entity information that Toefting and Bolton are Person and Organization entities is important. Extraction of these entities is in turn encouraged by the presence of the context words transferred to, which indicate an employment relation. Previous joint models have employed feature-based structured learning. An alternative approach to this end-to-end relation extraction task is to employ automatic feature learning via neural network (NN) based models.There are two ways to represent relations between entities using neural networks: recurrent/recursive neural networks (RNNs) and convolutional neural networks (CNNs). Among these, RNNs can directly represent essential linguistic structures, i.e., word sequences and constituent/dependency trees. Despite this representation ability, for relation classification tasks, the previously reported performance using long short-term memory (LSTM) based RNNs is worse than one using CNNs. These previous LSTM-based systems mostly include limited linguistic structures and neural architectures, and do not model entities and relations jointly. We are able to achieve improvements over state-of-the-art models via endto-end modeling of entities and relations based on richer LSTM-RNN architectures that incorporate complementary linguistic structures.Word sequence and tree structure are known to be complementary information for extracting relations. For instance, dependencies between words are not enough to predict that source and U.S. have an ORG-AFF relation in the sentence \"This is ...\", one U.S. source said, and the context word said is required for this prediction. Many traditional, feature-based relation classification models extract features from both sequences and parse trees. However, previous RNNbased models focus on only one of these linguistic structures.We present a novel end-to-end model to extract relations between entities on both word sequence and dependency tree structures. Our model allows joint modeling of entities and relations in a single model by using both bidirectional sequential (left-to-right and right-to-left) and bidirectional tree-structured (bottom-up and top-down) LSTM-RNNs. Our model first detects entities and then extracts relations between the detected entities using a single incrementally-decoded NN structure, and the NN parameters are jointly updated using both entity and relation labels. Unlike traditional incremental end-to-end relation extraction models, our model further incorporates two enhancements into training: entity pretraining, which pretrains the entity model, and scheduled sampling, which replaces (unreliable) predicted labels with gold labels in a certain probability. These enhancements alleviate the problem of low-performance entity detection in early stages of training, as well as allow entity information to further help downstream relation classification.On end-to-end relation extraction, we improve over the state-of-the-art feature-based model, with 12.1% (ACE2005) and 5.7% (ACE2004) relative error reductions in F1-score. On nominal relation classification (SemEval-2010 Task 8), our model compares favorably to the state-of-the-art CNNbased model in F1-score. Finally, we also ablate and compare our various model components, which leads to some key findings (both positive and negative) about the contribution and effectiveness of different RNN structures, input dependency relation structures, different parsing models, external resources, and joint learning settings.\n",
      "\n",
      "\n",
      "Joint extraction of entities and relations is to detect entity mentions and recognize their semantic relations simultaneously from unstructured text, as shows. Different from open information extraction (Open IE)) whose relation words are extracted from the given sentence, in this task, relation words are extracted from a predefined relation set which may not appear in the given sentence. It is an important issue in knowledge extraction and automatic construction of knowledge base. Traditional methods handle this task in a pipelined manner, i.e., extracting the entities first and then recognizing their relations. This separated framework makes the task easy to deal with, and each component can be more flexible. But it neglects the relevance between these two sub-tasks and each subtask is an independent model. The results of entity recognition may affect the performance of relation classification and lead to erroneous delivery.Different from the pipelined methods, joint learning framework is to extract entities together with relations using a single model. It can effectively integrate the information of entities and relations, and it has been shown to achieve better results in this task. However, most existing joint methods are feature-based structured systems. They need complicated feature engineering and heavily rely on the other NLP toolkits, which might also lead to error propagation. In order to reduce the manual work in feature extraction, recently, presents a neural network-based method for the end-to-end entities and relations extraction. Although the joint models can represent both entities and relations with shared parameters in a single model, they also extract the entities and relations separately and produce redundant information. For instance, the sentence in contains three entities: \"United States\", \"Trump\" and \"Apple Inc\". But only \"United States\" and \"Trump\" hold a fix relation \"Country-President\". Entity \"Apple Inc\" has no obvious relationship with the other entities in this sentence. Hence, the extracted result from this sentence is {United States e1 , Country-President r , Trump e2 }, which called triplet here.In this paper, we focus on the extraction of triplets that are composed of two entities and one relation between these two entities. Therefore, we can model the triplets directly, rather than extracting the entities and relations separately. Based on the motivations, we propose a tagging scheme accompanied with the end-to-end model to settle this problem. We design a kind of novel tags which contain the information of entities and the relationships they hold. Based on this tagging scheme, the joint extraction of entities and relations can be transformed into a tagging problem. In this way, we can also easily use neural networks to model the task without complicated feature engineering.Recently, end-to-end models based on LSTM have been successfully applied to various tagging tasks: Named Entity Recognition, CCG Supertagging, Chunking et al. LSTM is capable of learning long-term dependencies, which is beneficial to sequence modeling tasks. Therefore, based on our tagging scheme, we investigate different kinds of LSTM-based end-to-end models to jointly extract the entities and relations. We also modify the decoding method by adding a biased loss to make it more suitable for our special tags.The method we proposed is a supervised learning algorithm. In reality, however, the process of manually labeling a training set with a large number of entity and relation is too expensive and error-prone. Therefore, we conduct experiments on a public dataset 1 which is produced by distant supervision method to validate our approach. The experimental results show that our tagging scheme is effective in this task. In addition, our end-to-end model can achieve the best results on the public dataset.The major contributions of this paper are: (1) A novel tagging scheme is proposed to jointly extract entities and relations, which can easily transform the extraction problem into a tagging task. (2) Based on our tagging scheme, we study different kinds of end-to-end models to settle the problem. The tagging-based methods are better than most of the existing pipelined and joint learning methods. (3) Furthermore, we also develop an end-to-1 https://github.com/shanzhenren/CoType end model with biased loss function to suit for the novel tags. It can enhance the association between related entities.\n",
      "\n",
      "\n",
      "The goal of the entity recognition and relation extraction is to discover relational structures of entity mentions from unstructured texts. It is a central problem in information extraction since it is critical for tasks such as knowledge base population and question answering.The problem is traditionally approached as two separate subtasks, namely (i) named entity recognition (NER) and (ii) relation extraction (RE), in a pipeline setting. The main limitations of the pipeline models are: (i) error propagation between the components (i.e., NER and RE) and (ii) possible useful information from the one task is not exploited by the other (e.g., identifying a Works for relation might be helpful for the NER module in detecting the type of the two entities, i.e., PER, ORG and vice versa). On the other hand, more recent studies propose to use joint models to detect entities and their relations overcoming the aforementioned issues and achieving state-of-the-art performance.The previous joint models heavily rely on hand-crafted features. Recent advances in neural networks alleviate the issue of manual feature engineering, but some of them still depend on NLP tools (e.g., POS taggers, dependency parsers). propose a Recurrent Neural Network (RNN)-based joint model that uses a bidirectional sequential LSTM (Long Short Term Memory) to model the entities and a tree-LSTM that takes into account dependency tree information to model the relations between the entities. The dependency information is extracted using an external dependency parser. Similarly, in the work of for entity and relation extraction from biomedical text, a model which also uses tree-LSTMs is applied to extract dependency information. propose a method that relies on RNNs but uses a lot of hand-crafted features and additional NLP tools to extract features such as POS-tags, etc. replicate the context around the entities with Convolutional Neural Networks (CNNs). Note that the aforementioned works examine pairs of entities for relation extraction, rather than modeling the whole sentence directly. This means that relations of other pairs of entities in the same sentence -which could be helpful in deciding on the relation type fora particular pair -are not taken into account. propose a neural joint model based on LSTMs where they model the whole sentence at once, but still they do not have a principled way to deal with multiple relations. introduce a quadratic scoring layer to model the two tasks simultaneously. The limitation of this approach is that only a single relation can be assigned to a token, while the time complexity for the entity recognition task is increased compared to the standard approaches with linear complexity.In this work, we focus on anew general purpose joint model that performs the two tasks of entity recognition and relation extraction simultaneously, and that can handle multiple relations together. Our model achieves state-of-the-art performance in a number of different contexts (i.e., news, biomedical, real estate) and languages (i.e., English, Dutch) without relying on any manually engineered features nor additional NLP tools. In summary, our proposed model (which will be detailed next in Section 3) solves several shortcomings that we identified in related works (Section 2) for joint entity recognition and relation extraction: (i) our model does not rely on external NLP tools nor hand-crafted features, (ii) entities and relations within the same text fragment (typically a sentence) are extracted simultaneously, where (iii) an entity can be involved in multiple relations at once. Specifically, the model of depends on dependency parsers, which perform particularly well on specific languages (i.e., English) and contexts (i.e., news). Yet, our ambition is to develop a model that generalizes well in various setups, therefore using only automatically extracted features that are learned during training.For instance, and use exactly the same model in different contexts, i.e., news (ACE04) and biomedical data (ADE), respectively. Comparing our results to the ADE dataset, we obtain a 1.8% improvement on the NER task and ?3% on the RE task. On the other hand, our model performs within a reasonable margin (?0.6% in the NER task and ?1% on the RE task) on the ACE04 dataset without the use of pre-calculated features. This shows that the model of strongly relies on the features extracted by the dependency parsers and cannot generalize well into different contexts where dependency parser features are weak.Comparing to, we train our model by modeling all the entities and the relations of the sentence at once. This type of inference is beneficial in obtaining information about neighboring entities and relations instead of just examining a pair of entities each time. Finally, we solve the underlying problem of the models proposed by and, who essentially assume classes (i.e., relations) to be mutually exclusive: we solve this by phrasing the relation extraction component as a multi-label prediction problem. To demonstrate the effectiveness of the proposed method, we conduct the largest experimental evaluation to date (to the best of our knowledge) in jointly performing both entity recognition and relation extraction (see Section 4 and Section 5), using different datasets from various domains (i.e., news, biomedical, real estate) and languages (i.e., English, Dutch). Specifically, we apply our method to four datasets, namely ACE04 (news), Adverse Drug Events (ADE), Dutch Real Estate Classifieds (DREC) and CoNLL'04 (news). Our method outperforms all state-of-the-art methods that do not rely on any additional features or tools, while performance is very close (or even better in the biomedical dataset) compared to methods that do exploit hand-engineered features or NLP tools.\n",
      "\n",
      "\n",
      "Many neural network methods have recently been exploited in various natural language processing (NLP) tasks, such as parsing , POS tagging, relation extraction, translation, and joint tasks. However, observed that intentional small scale perturbations (i.e., adversarial examples) to the input of such models may lead to incorrect decisions (with high confidence). proposed adversarial training (AT) (for image recognition) as a regularization method which uses a mixture of clean and adversarial examples to enhance the robustness of the model. Although AT has recently been applied in NLP tasks (e.g., text classification), this paper -to the best of our knowledge -is the first attempt investigating regularization effects of AT in a joint setting for two related tasks.We start from a baseline joint model that performs the tasks of named entity recognition and relation extraction at once. Previously proposed models (summarized in Section 2) exhibit several issues that the neural network-based baseline approach (detailed in Section 3.1) overcomes: (i) our model uses automatically extracted features without the need of external parsers nor manually extracted features (see;;), (ii) all entities and the corresponding relations within the sentence are extracted at once, instead of examining one pair of entities at a time (see), and (iii) we model relation extraction in a multi-label setting, allowing multiple relations per entity (see;). The core contribution of the paper is the use of AT as an extension in the training procedure for the joint extraction task (Section 3.2).To evaluate the proposed AT method, we perform a large scale experimental study in this joint task (see Section 4), using datasets from different contexts (i.e., news, biomedical, real estate) and languages (i.e., English, Dutch). We use a strong baseline that outperforms all previous models that rely on automatically extracted features, achieving state-of-the-art performance (Section 5). Compared to the baseline model, applying AT during training leads to a consistent additional increase in joint extraction effectiveness.\n",
      "\n",
      "\n",
      "Relation extraction involves discerning whether a relation exists between two entities in a sentence (often termed subject and object, respectively). Successful relation extraction is the cornerstone of applications requiring relational understanding of unstructured text on a large scale, such as question answering, knowledge base population, and biomedical knowledge discovery.Models making use of dependency parses of the input sentences, or dependency-based models,: An example modified from the TAC KBP challenge corpus. A subtree of the original UD dependency tree between the subject (\"he\") and object (\"Mike Cane\") is also shown, where the shortest dependency path between the entities is highlighted in bold. Note that negation (\"not\") is off the dependency path.have proven to be very effective in relation extraction, because they capture long-range syntactic relations that are obscure from the surface form alone (e.g., when long clauses or complex scoping are present). Traditional feature-based models are able to represent dependency information by featurizing dependency trees as overlapping paths along the trees. However, these models face the challenge of sparse feature spaces and are brittle to lexical variations. More recent neural models address this problem with distributed representations built from their computation graphs formed along parse trees. One common approach to leverage dependency information is to perform bottom-up or top-down computation along the parse tree or the subtree below the lowest common ancestor (LCA) of the entities. Another popular approach, inspired by, is to reduce the parse tree to the shortest dependency path between the entities. However, these models suffer from several drawbacks. Neural models operating directly on parse trees are usually difficult to parallelize and thus computationally inefficient, because aligning trees for efficient batch training is usually nontrivial. Models based on the shortest dependency path between the subject and object are computationally more efficient, but this simplifying assumption has major limitations as well. shows a real-world example where crucial information (i.e., negation) would be excluded when the model is restricted to only considering the dependency path.In this work, we propose a novel extension of the graph convolutional network) that is tailored for relation extraction. Our model encodes the dependency structure over the input sentence with efficient graph convolution operations, then extracts entity-centric representations to make robust relation predictions. We also apply a novel path-centric pruning technique to remove irrelevant information from the tree while maximally keeping relevant content, which further improves the performance of several dependencybased models including ours. We test our model on the popular SemEval 2010 Task 8 dataset and the more recent, larger TAC-RED dataset. On both datasets, our model not only outperforms existing dependency-based neural models by a significant margin when combined with the new pruning technique, but also achieves a 10-100x speedup over existing tree-based models. On TACRED, our model further achieves the state-of-the-art performance, surpassing a competitive neural sequence model baseline. This model also exhibits complementary strengths to sequence models on TACRED, and combining these two model types through simple prediction interpolation further improves the state of the art.To recap, our main contributions are: (i) we propose a neural model for relation extraction based on graph convolutional networks, which allows it to efficiently pool information over arbitrary dependency structures; (ii) we present anew pathcentric pruning technique to help dependencybased models maximally remove irrelevant information without damaging crucial content to improve their robustness; (iii) we present detailed analysis on the model and the pruning technique, and show that dependency-based models have complementary strengths with sequence models.\n",
      "\n",
      "\n",
      "Extracting entities and their semantic relations from raw text is a key information extraction task. For example, given the sentence \" David Foster is the AP 's Northwest regional reporter , based in Seattle \" in the CoNLL04 dataset, our goal is to recognize \"David Foster\" as person, \"AP\" as organization, and \"Northwest\" and \"Seattle\" as location entities, then classifiy entity pairs to extract structured information: Work For(David Foster, AP), OrgBased In(AP, Northwest) and OrgBased In(AP, Seattle). Such information is useful in many other NLP tasks. Especially in IR applications such as entity search, structured search and question answering, it helps provide end users with significantly better search experience.A common relation extraction approach is to construct pipeline systems with separate sub-systems for the two tasks of named entity recognition and relation classification. More recently, end-to-end systems which jointly learn to extract entities and relations have been proposed with strong potential to obtain high performance. Traditional joint approaches are feature-based supervised learning methods which employ numerous syntactic and lexical features based on external NLP tools as well as knowledge base resources.State-of-the-art relation extraction performance has been obtained by end-to-end models based on neural networks. Specifically, proposed a RNNbased model which achieved top results on the CoNLL04 dataset. Their approach relies on various manually extracted features. Other neural models employ dependency parsing-based information. In particular, applied bottom-up and top-down tree-structured LSTMs to model dependency paths between entities. integrated implicit syntactic information by using latent feature representations extracted from a pre-trained BiLSTM-based dependency parser.  entity recognition, and a CNN on top of the BiLSTM for classifying relations. Adel and Schütze (2017) assumed that entity boundaries are given, and trained a CNN to extract context features around the entities, and using these features for entity and relation classification. Recently, formulated the joint entity and relation extraction problem as a directed graph and proposed a BiLSTM-and transition-based approach to generate the graph incrementally.  [4] extended the multi-head selection-based joint model with adversarial training. In, the joint task is formulated as a sequence tagging problem, and a BiLSTM with a softmax output layer can then be used for joint prediction.In this paper, we present a novel end-to-end neural model for joint entity and relation extraction. As illustrated in, our model architecture can be viewed as a mixture of a named entity recognition (NER) component and a relation classification (RC) component. Our NER component employs a BiLSTM-CRF architecture to predict entities from input word tokens. Based on both the input words and the predicted NER labels, the RC component uses another BiLSTM to learn latent features relevant for relation classification. In most previous neural joint models, the relation classification part relies on a common \"linear\" concatenation-based mechanism over the latent features associated with entity pairs, i.e. the latent features are first concatenated into a single feature vector which is then linearly transformed before being fed into a softmax classifier. In contrast, our RC component takes into account second-order interactions over the latent features via a tensor. In particular, for relation classification we propose a novel use of the deep biaffine attention mechanism which was first introduced in dependency parsing.Experimental results on the benchmark \"relation and entity recognition\" dataset CoNLL04 show that our model outperforms previous models, obtaining new stateof-the-art scores. In addition, using the biaffine attention improves the performance compared to using the linear mechanism significantly. We also provide an ablation study to investigate effects of different contributing factors in our model.\n",
      "\n",
      "\n",
      "Classifying semantic relations between entity pairs in sentences plays a vital role in various NLP tasks, such as information extraction, question answering and knowledge base population. A task of relation classification is defined as predicting a semantic relationship between two tagged entities in a sentence. For example, given a sentence with tagged entity pair, crash and attack, this sentence is classified into the re-lation Cause-Effect(e1,e2) 1 between the entity pair like. A first entity is surrounded by e1 and /e1 , and a second entity is surrounded by e2 and /e2 .Most previous relation classification models rely heavily on high-level lexical and syntactic features obtained from NLP tools such as WordNet, dependency parser, part-of-speech (POS) tagger, and named entity recognizer (NER). The classification models relying on such features suffer from propagation of implicit error of the tools and they are computationally expensive.Recently, many studies therefore propose end-toend neural models without the high-level features. Among them, attention-based models, which focus to the most important semantic information in a sentence, show state-of-the-art results in a lot of NLP tasks. Since these models are mainly proposed for solving translation and language modeling tasks, they could not fully utilize the information of tagged entities in relation classification task. However, tagged entity pairs could be powerful hints for solving relation classification task. For example, even if we do not consider other words except the crash and attack, we intuitively know that the entity pair has a relation Cause-Effect(e1,e2) 1 better than Component-Whole(e1,e2) 1 in To address these issues, We propose a novel endto-end recurrent neural model which incorporates an entity-aware attention mechanism with a latent entity typing (LET). To capture the context of sentences, We obtain word representations by self attention mechanisms and build the recurrent neural architecture with Bidirectional Long Short-Term Memory (LSTM) networks. Entity-aware attention focuses on the most important semantic information considering entity pairs with word positions relative to these pairs and latent types obtained by LET.The contributions of our work are summarized as follows: We propose an novel end-to-end recurrent neural model and an entity-aware attention mechanism with a LET which focuses to semantic information of entities and their latent types; (2) Our model obtains 85.2% F1-score in SemEval-2010 Task 8 and it outper- forms existing state-of-the-art models without any highlevel features; We show that our model is more interpretable since it's decision making process could be visualized with self attention, entity-aware attention, and LET.\n",
      "\n",
      "\n",
      "The volume of biomedical literature continues to rapidly increase. On average, more than 3000 new articles are published everyday in peer-reviewed journals, excluding pre-prints and technical reports such as clinical trial reports in various archives. PubMed alone has a total of 29M articles as of January 2019. Reports containing valuable information about new discoveries and new insights are continuously added to the already overwhelming amount of literature. Consequently, there is increasingly more demand for accurate biomedical text mining tools for extracting information from the literature.Recent progress of biomedical text mining models was made possible by the advancements of deep learning techniques used in natural language processing (NLP). For instance, Long Short-Term Memory (LSTM) and Conditional Random Field (CRF) have greatly improved performance in biomedical named entity recognition (NER) over the last few years. Other deep learning based models have made improvements in biomedical text mining tasks such as relation extraction (RE) and question answering (QA).However, directly applying state-of-the-art NLP methodologies to biomedical text mining has limitations. First, as recent word representation models such as Word2Vec, ELMo and BERT are trained and tested mainly on datasets containing general domain texts (e.g. Wikipedia), it is difficult to estimate their performance on datasets containing biomedical texts. Also, the word distributions of general and biomedical corpora are quite different, which can often be a problem for biomedical text mining models. As a result, recent models in biomedical text mining rely largely on adapted versions of word representations.In this study, we hypothesize that current state-of-the-art word representation models such as BERT need to be trained on biomedical corpora to be effective in biomedical text mining tasks. Previously, Word2Vec, which is one of the most widely known context independent word representation models, was trained on biomedical corpora which contain terms and expressions that are usually not included in a general domain corpus. While ELMo and BERT have proven the effectiveness of contextualized word representations, they cannot obtain high performance on biomedical corpora because they are pre-trained on only general domain corpora. As BERT achieves very strong results on various NLP tasks while using almost the same structure across the tasks, adapting BERT for the biomedical domain could potentially benefit numerous biomedical NLP researches.\n",
      "\n",
      "\n",
      "Relation extraction (RE) aims to find the semantic relation between a pair of entity mentions from an input paragraph. A solution to this task is essential for many downstream NLP applications such as automatic knowledge-base completion, knowledge base question answering, and symbolic approaches for visual question answering, etc.One particular type of the RE task is multiplerelations extraction (MRE) that aims to recognize relations of multiple pairs of entity mentions from an input paragraph. Because in real-world applications, whose input paragraphs dominantly contain multiple pairs of entities, an efficient and effective solution for MRE has more important and more practical implications. However, nearly all existing approaches for MRE tasks  2014; adopt some variations of the singlerelation extraction (SRE) approach, which treats each pair of entity mentions as an independent instance, and requires multiple passes of encoding for the multiple pairs of entities. The drawback of this approach is obvious -it is computationally expensive and this issue becomes more severe when the input paragraph is large, making this solution impossible to implement when the encoding step involves deep models.This work presents a solution that can resolve the inefficient multiple-passes issue of existing solutions for MRE by encoding the input only once, which significantly increases the efficiency and scalability. Specifically, the proposed solution is built on top of the existing transformer-based, pretrained general-purposed language encoders. In this paper we use Bidirectional Encoder Representations from Transformers (BERT) as the transformer-based encoder, but this solution is not limited to using BERT alone. The two novel modifications to the original BERT architecture are: (1) we introduce a structured prediction layer for predicting multiple relations for different entity pairs; and (2) we make the selfattention layers aware of the positions of all en-tities in the input paragraph. To the best of our knowledge, this work is the first promising solution that can solve MRE tasks with such high efficiency (encoding the input in one-pass) and effectiveness (achieve anew state-of-the-art performance), as proved on the ACE 2005 benchmark.\n",
      "\n",
      "\n",
      "The exponential increase in the volume of scientific publications in the past decades has made NLP an essential tool for large-scale knowledge extraction and machine reading of these documents. Recent progress in NLP has been driven by the adoption of deep neural models, but training such models often requires large amounts of labeled data. In general domains, large-scale training data is often possible to obtain through crowdsourcing, but in scientific domains, annotated data is difficult and expensive to collect due to the expertise required for quality annotation.As shown through ELMo , and BERT, unsupervised pretraining of language models on large corpora significantly improves performance on many NLP tasks. These models return contextualized embeddings for each token which can be passed into minimal task-specific neural architectures. Leveraging the success of unsupervised pretraining has become especially important especially when task-specific annotations are difficult to obtain, like in scientific NLP. Yet while both BERT and ELMo have released pretrained models, they are still trained on general domain corpora such as news articles and Wikipedia.In this work, we make the following contributions:(i) We release SCIBERT, anew resource demonstrated to improve performance on a range of NLP tasks in the scientific domain. SCIBERT is a pretrained language model based on BERT but trained on a large corpus of scientific text.(ii) We perform extensive experimentation to investigate the performance of finetuning versus task-specific architectures atop frozen embeddings, and the effect of having an in-domain vocabulary.(iii) We evaluate SCIBERT on a suite of tasks in the scientific domain, and achieve new state-ofthe-art (SOTA) results on many of these tasks.\n",
      "\n",
      "\n",
      "Text classification is a classic topic for natural language processing, in which one needs to assign predefined categories to free-text documents. The range of text classification research goes from designing the best features to choosing the best possible machine learning classifiers. To date, almost all techniques of text classification are based on words, in which simple statistics of some ordered word combinations (such as n-grams) usually perform the best.On the other hand, many researchers have found convolutional networks (ConvNets) are useful in extracting information from raw signals, ranging from computer vision applications to speech recognition and others. In particular, time-delay networks used in the early days of deep learning research are essentially convolutional networks that model sequential data.In this article we explore treating text as a kind of raw signal at character level, and applying temporal (one-dimensional) ConvNets to it. For this article we only used a classification task as away to exemplify ConvNets' ability to understand texts. Historically we know that ConvNets usually require large-scale datasets to work, therefore we also build several of them. An extensive set of comparisons is offered with traditional models and other deep learning models.Applying convolutional networks to text classification or natural language processing at large was explored in literature. It has been shown that ConvNets can be directly applied to distributed or discrete embedding of words, without any knowledge on the syntactic or semantic structures of a language. These approaches have been proven to be competitive to traditional models. from previous research that ConvNets do not require the knowledge about the syntactic or semantic structure of a language. This simplification of engineering could be crucial fora single system that can work for different languages, since characters always constitute a necessary construct regardless of whether segmentation into words is possible. Working on only characters also has the advantage that abnormal character combinations such as misspellings and emoticons maybe naturally learnt.\n",
      "\n",
      "\n",
      "Text categorization is the task of assigning labels to documents written in a natural language, and it has numerous real-world applications including sentiment analysis as well as traditional topic assignment tasks. The state-of-the art methods for text categorization had long been linear predictors (e.g., SVM with a linear kernel) with either bag-ofword or bag-of-n-gram vectors (hereafter bow) as input, e.g.,. This, however, A convolutional neural network (CNN)) is a feedforward neural network with convolution layers interleaved with pooling layers, originally developed for image processing. In its convolution layer, a small region of data (e.g., a small square of image) at every location is converted to a low-dimensional vector with information relevant to the task being preserved, which we loosely term 'embedding'. The embedding function is shared among all the locations, so that useful features can be detected irrespective of their locations. In its simplest form, onehot CNN works as follows. A document is represented as a sequence of one-hot vectors (each of which indicates a word by the position of a 1); a convolution layer converts small regions of the document (e.g., \"I love it\") to low-dimensional vectors at every location (embedding of text regions); a pooling layer aggregates the region embedding results to a document vector by taking componentwise maximum or average; and the top layer classifies a document vector with a linear model. The onehot CNN and its semi-supervised extension were shown to be superior to a number of previous methods.In this work, we consider a more general framework (subsuming one-hot CNN) which jointly trains a feature generator and a linear model, where the feature generator consists of 'region embedding + pooling'. The specific region embedding function of one-hot CNN takes the simple form v(x ) = max(0, Wx + b) ,where x is a concatenation of one-hot vectors (therefore, 'one-hot' in the name) of the words in the -th region (of a fixed size), and the weight matrix W and the bias vector b need to be trained. It is simple and fast to compute, and considering its simplicity, the method works surprisingly well if the region size is appropriately set. However, there are also potential shortcomings. The region size must be fixed, which may not be optimal as the size of relevant regions may vary. Practically, the region size cannot be very large as the number of parameters to be learned (components of W) depends on it. JZ15 proposed variations to alleviate these issues. For example, a bow-input variation allows x above to be a bow vector of the region. This enables a larger region, but at the expense of losing word order in the region and so its use maybe limited.In this work, we build on the general framework of 'region embedding + pooling' and explore a more sophisticated region embedding via Long Short-Term Memory (LSTM), seeking to overcome the shortcomings above, in the supervised and semi-supervised settings. LSTM) is a recurrent neural network. In its typical applications to text, an LSTM takes words in a sequence one by one; i.e., at time t, it takes as input the t-th word and the output from time t ? 1. Therefore, the output from each time step can be regarded as the embedding of the sequence of words that have been seen so far (or a relevant part of it). It is designed to enable learning of dependencies over larger time lags than feasible with traditional recurrent networks. That is, an LSTM can be used to embed text regions of variable (and possibly large) sizes.We pursue the best use of LSTM for our purpose, and then compare the resulting model with the previous best methods including one-hot CNN and previous LSTM. Our strategy is to simplify the model as much as possible, including elimination of a word embedding layer routinely used to produce input to LSTM. Our findings are threefold. First, in the supervised setting, our simplification strategy leads to higher accuracy and faster training than previous LSTM. Second, accuracy can be further improved by training LSTMs on unlabeled data for learning useful region embeddings and using them to produce additional input. Third, both our LSTM models and one-hot CNN strongly outperform other methods including previous LSTM. The best results are obtained by combining the two types of region embeddings (LSTM embed-dings and CNN embeddings) trained on unlabeled data, indicating that their strengths are complementary. Overall, our results show that for text categorization, embeddings of text regions, which can convey higher-level concepts than single words in isolation, are useful, and that useful region embeddings can be learned without going through word embedding learning. We report performances exceeding the previous best results on four benchmark datasets. Our code and experimental details are available at http://riejohnson.com/cnn download.html.\n",
      "\n",
      "\n",
      "Text classification is an important task in Natural Language Processing with many applications, such as web search, information retrieval, ranking and document classification.Recently, models based on neural networks have become increasingly popular. While these models achieve very good performance in practice, they tend to be relatively slow both at train and test time, limiting their use on very large datasets.Meanwhile, linear classifiers are often considered as strong baselines for text classification problems. Despite their simplicity, they often obtain stateof-the-art performances if the right features are used.They also have the potential to scale to very large corpus.In this work, we explore ways to scale these baselines to very large corpus with a large output space, in the context of text classification. Inspired by the recent work in efficient word representation learning, we show that linear models with a rank constraint and a fast loss approximation can train on a billion words within ten minutes, while achieving performance on par with the state-of-the-art. We evaluate the quality of our approach fastText 1 on two different tasks, namely tag prediction and sentiment analysis.\n",
      "\n",
      "\n",
      "Words are often considered as the basic constituents of texts for many languages, including English. The first module in an NLP pipeline is a tokenizer which transforms texts to sequences of words.However, in practise, other preprocessing techniques can be (and are) further used together with tokenization. These include lemmatization, lowercasing and 1 Note that although word-based models are mainstream in NLP in general and text classification in particular, recent work has also considered other linguistic units, such as characters or word senses. These techniques require a different kind of preprocessing and, while they have been shown effective in various settings, in this work we only focus on the mainstream word-based models. multiword grouping, among others. Although these preprocessing decisions have been studied in the context of conventional text classification techniques, little attention has been paid to them in the more recent neural-based models. The most similar study to ours is, which analyzed different encoding levels for English and Asian languages such as Chinese, Japanese and Korean. As opposed to our work, their analysis was focused on UTF-8 bytes, characters, words, romanized characters and romanized words as encoding levels, rather than the preprocessing techniques analyzed in this paper.Additionally, word embeddings have been shown to play an important role in boosting the generalization capabilities of neural systems. However, while some studies have focused on intrinsically analyzing the role of lemmatization in their underlying training corpus, the impact on their extrinsic performance when integrated into a neural network architecture has remained understudied. In this paper we focus on the role of preprocessing the input text, particularly in how it is split into individual (meaning-bearing) tokens and how it affects the performance of standard neural text classification models based on. CNNs have proven to be effective in a wide range of NLP applications, in-cluding text classification tasks such as topic categorization and polarity detection, which are the tasks considered in this work. The goal of our evaluation study is to find answers to the following two questions:1. Are neural network architectures (in particular CNNs) affected by seemingly small preprocessing decisions in the input text?2. Does the preprocessing of the embeddings' underlying training corpus have an impact on the final performance of a state-of-the-art neural network text classifier?According to our experiments in topic categorization and polarity detection, these decisions are important in certain cases. Moreover, we shed some light on the motivations of each preprocessing decision and provide some hints on how to normalize the input corpus to better suit each setting.The accompanying materials of this submission can be downloaded at the following repository: github.com/pedrada88/preproc-textclassification.\n",
      "\n",
      "\n",
      "In the last few years, convolutional neural networks (CNNs) have demonstrated remarkable progress in various natural language processing applications, including sentence/document classification, text sequence matching, generic text representations, language modeling , machine translation and abstractive sentence summarization. CNNs are typically applied to tasks where feature extrac-tion and a corresponding supervised task are approached jointly. As an encoder network for text, CNNs typically convolve a set of filters, of window size n, with an inputsentence embedding matrix obtained via word2vec or Glove. Different filter sizes n maybe used within the same model, exploiting meaningful semantic features from different n-gram fragments.The learned weights of CNN filters, inmost cases, are assumed to be fixed regardless of the input text. As a result, the rich contextual information inherent in natural language sequences may not be fully captured. As demonstrated in, the context of a word tends to greatly influence its contribution to the final supervised tasks. This observation is consistent with the following intuition: when reading different types of documents, e.g., academic papers or newspaper articles, people tend to adopt distinct strategies for better and more effective understanding, leveraging the fact that the same words or phrases may have different meaning or imply different things, depending on context.Several research efforts have sought to incorporate contextual information into CNNs to adaptively extract text representations. One common strategy is the attention mechanism, which is typically employed on top of a CNN (or Long Short-Term Memory (LSTM)) layer to guide the extraction of semantic features. For the embedding of a single sentence, proposed a selfattentive model that attends to different parts of a sentence and combines them into multiple vector representations. However, their model needs considerably more parameters to achieve performance gains over traditional CNNs. To match sentence pairs, introduced an attentionbased CNN model, which re-weights the convolution inputs or outputs, to extract interdepen-dent sentence representations.; explore a compare and aggregate framework to directly capture the wordby-word matching between two paired sentences. However, these approaches suffer from the problem of high matching complexity, since a similarity matrix between pairwise words needs to be computed, and thus it is computationally inefficient or even prohibitive when applied to long sentences.In this paper, we propose a generic approach to learn context-sensitive convolutional filters for natural language understanding. In contrast to traditional CNNs, the convolution operation in our framework does not have a fixed set of filters, and thus provides the network with stronger modeling flexibility and capacity. Specifically, we introduce a meta network to generate a set of contextsensitive filters, conditioned on specific input sentences; these filters are adaptively applied to either the same (Section 3.2) or different (Section 3.3) text sequences. In this manner, the learned filters vary from sentence to sentence and allow for more fine-grained feature abstraction.Moreover, since the generated filters in our framework can adapt to different conditional information available (labels or paired sentences), they can be naturally generalized to model sentence pairs. In this regard, we propose a novel bidirectional filter generation mechanism to allow interactions between sentence pairs while constructing context-sensitive representations.We investigate the effectiveness of our Adaptive Context-sensitive CNN (ACNN) framework on several text processing tasks: ontology classification, sentiment analysis, answer sentence selection and paraphrase identification. We show that the proposed methods consistently outperforms the standard CNN and attention-based CNN baselines. Our work provides anew perspective on how to incorporate contextual information into text representations, which can be combined with more sophisticated structures to achieve even better performance in the future.\n",
      "\n",
      "\n",
      "Inductive transfer learning has had a large impact on computer vision (CV). Applied CV models (including object detection, classification, and segmentation) are rarely trained from scratch, but instead are fine-tuned from models that have been pretrained on ImageNet, MS-COCO, and other datasets.Text classification is a category of Natural Language Processing (NLP) tasks with real-world applications such as spam, fraud, and bot detection, emergency response, and commercial document classification, such as for legal discovery. 1 http://nlp.fast.ai/ulmfit. Equal contribution. Jeremy focused on the algorithm development and implementation, Sebastian focused on the experiments and writing.While Deep Learning models have achieved state-of-the-art on many NLP tasks, these models are trained from scratch, requiring large datasets, and days to converge. Research in NLP focused mostly on transductive transfer. For inductive transfer, fine-tuning pretrained word embeddings, a simple transfer technique that only targets a model's first layer, has had a large impact in practice and is used inmost state-of-the-art models. Recent approaches that concatenate embeddings derived from other tasks with the input at different layers) still train the main task model from scratch and treat pretrained embeddings as fixed parameters, limiting their usefulness.In light of the benefits of pretraining, we should be able to do better than randomly initializing the remaining parameters of our models. However, inductive transfer via finetuning has been unsuccessful for NLP. first proposed finetuning a language model (LM) but require millions of in-domain documents to achieve good performance, which severely limits its applicability.We show that not the idea of LM fine-tuning but our lack of knowledge of how to train them effectively has been hindering wider adoption. LMs overfit to small datasets and suffered catastrophic forgetting when fine-tuned with a classifier. Compared to CV, NLP models are typically more shallow and thus require different fine-tuning methods.We propose anew method, Universal Language Model Fine-tuning (ULMFiT) that addresses these issues and enables robust inductive transfer learning for any NLP task, akin to fine-tuning ImageNet models: The same 3-layer LSTM architecturewith the same hyperparameters and no additions other than tuned dropout hyperparametersoutperforms highly engineered models and trans-fer learning approaches on six widely studied text classification tasks. On IMDb, with 100 labeled examples, ULMFiT matches the performance of training from scratch with 10× and-given 50k unlabeled examples-with 100× more data.Contributions Our contributions are the following: 1) We propose Universal Language Model Fine-tuning (ULMFiT), a method that can be used to achieve CV-like transfer learning for any task for NLP. 2) We propose discriminative fine-tuning, slanted triangular learning rates, and gradual unfreezing, novel techniques to retain previous knowledge and avoid catastrophic forgetting during fine-tuning. 3) We significantly outperform the state-of-the-art on six representative text classification datasets, with an error reduction of 18-24% on the majority of datasets. 4) We show that our method enables extremely sample-efficient transfer learning and perform an extensive ablation analysis. 5) We make the pretrained models and our code available to enable wider adoption.\n",
      "\n",
      "\n",
      "Limited amounts of training data are available for many NLP tasks. This presents a challenge for data hungry deep learning methods. Given the high cost of annotating supervised training data, very large training sets are usually not available for most research or industry NLP tasks. Many models address the problem by implicitly performing limited transfer learning through the use of pre-trained word embeddings such as those produced by word2vec or GloVe. However, recent work has demonstrated strong transfer task performance using pre-trained sentence level embeddings.In this paper, we present two models for producing sentence embeddings that demonstrate good transfer to a number of other of other NLP tasks. We include experiments with varying amounts of transfer task training data to illustrate the relationship between transfer task performance and training set size. We find that our sentence embeddings can be used to obtain surprisingly good task performance with remarkably little task specific training data. The sentence encoding models are made publicly available on TF Hub.Engineering characteristics of models used for transfer learning are an important consideration. We discuss modeling trade-offs regarding memory requirements as well as compute time on CPU and GPU. Resource consumption comparisons are made for sentences of varying lengths. import tensorflow_hub as hub embed = hub.Module(\"https://tfhub.dev/google/\" \"universal-sentence-encoder/1\") embedding = embed([ \"The quick brown fox jumps over the lazy dog.\"])Listing 1: Python example code for using the universal sentence encoder.\n",
      "\n",
      "\n",
      "Modeling articles or sentences computationally is a fundamental topic in natural language processing. It could be as simple as a keyword/phrase matching problem, but it could also be a nontrivial problem if compositions, hierarchies, and structures of texts are considered. For example, a news article which mentions a single phrase \"US election\" maybe categorized into the political news with high probability. But it could be very difficult fora computer to predict which presidential candidate is favored by its author, or whether the author's view in the article is more liberal or more conservative.Earlier efforts in modeling texts have achieved limited success on text categorization using a simple bag-of-words classifier, implying understanding the meaning of the individual word or n-gram is a necessary step towards more sophisticated models. It is therefore not a surprise that distributed representations of words, a.k.a. word embeddings, have received great attention from NLP community addressing the question \"what\" to be modeled at the basic level. In order to model higher level concepts and facts in texts, an NLP researcher has to think cautiously the so-called \"what\" question: what is actually modeled beyond word meanings. A common approach to the question is to treat the texts as sequences and focus on their spatial patterns, whose representatives include convolutional neural networks (CNNs) and long shortterm memory networks (LSTMs). Another common approach is to completely ignore the order of words but focus on their compositions as a collection, whose representatives include probabilistic topic modeling and Earth Mover's Distance based modeling.Those two approaches, albeit quite different from the computational perspective, actually follow a common measure to be diagnosed regarding their answers to the \"what\" question. In neural network approaches, spatial patterns aggregated at lower levels contribute to representing higher level concepts. Here, they form a recursive process to articulate what to be modeled. For example, CNN builds convolutional feature detectors to extract local patterns from a window of vector sequences and uses max-pooling to select the most prominent ones. It then hierarchically builds such pattern extraction pipelines at multiple levels. Being a spatially sensitive model, CNN pays a price for the inefficiency of replicating feature detectors on a grid. As argued in, one has to choose between replicating detectors whose size grows exponentially with the number of dimensions, or increasing the volume of the labeled training set in a similar exponential way. On the other hand, methods that are spatially insensitive are perfectly efficient at the inference time regardless of any order of words or local patterns. However, they are unavoidably more restricted to encode rich structures presented in a sequence. Improving the efficiency to encode spatial patterns while keeping the flexibility of their representation capability is thus a central issue.A recent method called capsule network introduced by possesses this attractive potential to address the aforementioned issue. They introduce an iterative routing process to decide the credit attribution between nodes from lower and higher layers. A metaphor (also as an argument) they made is that human visual system intelligently assigns parts to wholes at the inference time without hard-coding patterns to be perspective relevant. As an outcome, their model could encode the intrinsic spatial relationship between apart and a whole constituting viewpoint invariant knowledge that automatically generalizes to novel viewpoints. In our work, we follow a similar spirit to use this technique in modeling texts. Three strategies are proposed to stabilize the dynamic routing process to alleviate the disturbance of some noise capsules which may contain \"background\" information such as stop words and the words that are unrelated to specific categories. We conduct a series of experiments with capsule networks on top of the pre-trained word vectors for six text classification benchmarks. More importantly, we show that capsule networks achieves significant improvement when transferring singlelabel to multi-label text classifications over strong baseline methods.\n",
      "\n",
      "\n",
      "Word embeddings, learned from massive unstructured text data, are widely-adopted building blocks for Natural Language Processing (NLP). By representing each word as a fixed-length vector, these embeddings can group semantically similar words, while implicitly encoding rich linguis-tic regularities and patterns. Leveraging the word-embedding construct, many deep architectures have been proposed to model the compositionality in variable-length text sequences. These methods range from simple operations like addition, to more sophisticated compositional functions such as Recurrent Neural Networks (RNNs), Convolutional Neural Networks (CNNs) and Recursive Neural Networks.Models with more expressive compositional functions, e.g., RNNs or CNNs, have demonstrated impressive results; however, they are typically computationally expensive, due to the need to estimate hundreds of thousands, if not millions, of parameters. In contrast, models with simple compositional functions often compute a sentence or document embedding by simply adding, or averaging, over the word embedding of each sequence element obtained via, e.g., word2vec, or GloVe. Generally, such a Simple Word-Embedding-based Model (SWEM) does not explicitly account for spatial, word-order information within a text sequence. However, they possess the desirable property of having significantly fewer parameters, enjoying much faster training, relative to RNN-or CNN-based models. Hence, there is a computation-vs.-expressiveness tradeoff regarding how to model the compositionality of a text sequence.In this paper, we conduct an extensive experimental investigation to understand when, and why, simple pooling strategies, operated over word embeddings alone, already carry sufficient information for natural language understanding. To ac-count for the distinct nature of various NLP tasks that may require different semantic features, we compare SWEM-based models with existing recurrent and convolutional networks in a pointby-point manner. Specifically, we consider 17 datasets, including three distinct NLP tasks: document classification (Yahoo news, Yelp reviews, etc.), natural language sequence matching (SNLI, WikiQA, etc.) and (short) sentence classification/tagging (Stanford sentiment treebank,. Surprisingly, SWEMs exhibit comparable or even superior performance in the majority of cases considered.In order to validate our experimental findings, we conduct additional investigations to understand to what extent the word-order information is utilized/required to make predictions on different tasks. We observe that in text representation tasks, many words (e.g., stop words, or words that are not related to sentiment or topic) do not meaningfully contribute to the final predictions (e.g., sentiment label). Based upon this understanding, we propose to leverage a max-pooling operation directly over the word embedding matrix of a given sequence, to select its most salient features. This strategy is demonstrated to extract complementary features relative to the standard averaging operation, while resulting in a more interpretable model. Inspired by a case study on sentiment analysis tasks, we further propose a hierarchical pooling strategy to abstract and preserve the spatial information in the final representations. This strategy is demonstrated to exhibit comparable empirical results to LSTM and CNN on tasks that are sensitive to word-order features, while maintaining the favorable properties of not having compositional parameters, thus fast training.Our work presents a simple yet strong baseline for text representation learning that is widely ignored in benchmarks, and highlights the general computation-vs.-expressiveness tradeoff associated with appropriately selecting compositional functions for distinct NLP problems. Furthermore, we quantitatively show that the word-embeddingbased text classification tasks can have the similar level of difficulty regardless of the employed models, using the subspace training to constrain the trainable parameters. Thus, according to Occam's razor, simple models are preferred.\n",
      "\n",
      "\n",
      "One of the primary tasks in natural language processing (NLP) is sentence classification, where given a sentence (e.g. a sentence of a review) as input, we are tasked to classify it into one of multiple classes (e.g. into positive or negative). This task is important as it is widely used in almost all subareas of NLP such as sentiment classification for sentiment analysis and question type classification for question answering, to name a few. While past methods require feature engineering, recent methods enjoy neural-based methods to automatically encode the sentences into low-dimensional dense vectors. Despite the success of these methods, the major challenge in this task is that extracting features from a single sentence limits the performance.To overcome this limitation, recent works attempted to augment different kinds of features to the sentence, such as the neighboring sentences and the topics of the sentences. However, these methods used domain-dependent contexts that are only effective when the domain of the task is appropriate. For one thing, neighboring sentences may not be available in some tasks such as question type classification. Moreover, topics inferred using topic models may produce less useful topics when the data set is domain-specific such as movie review sentiment classification.In this paper, we propose the usage of translations as compelling and effective domain-free contexts, or contexts that are always available no matter what the task domain is. We observe two opportunities when using translations.First, each language has its own linguistic and cultural characteristics that may contain different signals to effectively classify a specific class. contrasts the sentence vectors of the original English sentences and their Arabictranslated sentences in the question type classification task. A yellow circle signifies a clear separation of a class. For example, the green class, or the numeric question type, is circled in the Arabic space as it is clearly separated from other classes, while such separation cannot be observed in English. Meanwhile, location type questions (in orange) are better classified in English.Second, the original sentences may include languagespecific ambiguity, which maybe resolved when presented with its translations. Consider the example English sentence \"The movie is terribly amazing\" for the sentiment classification task. In this case, terribly can be used in both positive and negative sense, thus introduces ambiguity in the sentence. When translated to Korean, it becomes \"? ? ?? ?? ? ? ? ?? ? ?? ? ? ? ? ? ? ?? ? ?? ?? ?\" which means \"The movie is greatly magnificent\", removing the ambiguity.The above two observations hold only when translations are supported for (nearly) arbitrary language pairs with sufficiently high quality. Thankfully, translation services (e.g. Google Translate) Moreover, recent research on neural machine translation (NMT) improved the efficiency and even enabled zero-shot translation of models for languages with no parallel data. This provides an opportunity to leverage on as many languages as possible to any domain, providing a much wider context compared to the limited contexts provided by past studies.However, despite the maturity of translation, naively concatenating their vectors to the original sentence vector may introduce more noise than signals. The unaltered translation space on the left of shows an example where translation noises make the two classes indistinguishable.In this paper, we propose a method to mitigate the possible problems when using translated sentences as context based on the following observations. Suppose there are two translated sentences a and b with slight errors. We posit that a can be used to fix b when a is used as a context of b, and vice versa 1 . Revisiting the example above, to fix the vector of the English sentence \"The movie is terribly amazing\", we use the Korean translation to move the vector towards the location where the vector \"The movie is greatly magnificent\" is.Based on these observations, we present a neural attentionbased multiple context fixing attachment (MCFA). MCFA is a series of modules that uses all the sentence vectors (e.g. Arabic, English, Korean, etc.) as context to fix a sentence vector (e.g. Korean). Fixing the vectors is done by selectively moving the vectors to a location in the same vector space that better separates the class, as shown in. Noises from translation may cause adverse effects to the vector itself (e.g. when a noisy vector is directly used for the task) and relatively to other vectors (e.g. when a noisy vector is used to fix another noisy vector). MCFA computes two sentence usability metrics to control the noise when fixing vectors: (a) self usability ? i (a) weighs the confidence of using sentence a in solving the task. (b) relative usability ? r (a, b) weighs the confidence of using sentence a in fixing sentence b.Listed below are the three main strengths of the MCFA attachment. (1) MCFA is attached after encoding the sentence, which makes it widely adaptable to other models. (2) MCFA is extensible and improves the accuracy as the number of translated sentences increases. (3) MCFA moves the vectors inside the same space, thus preserves the meaning of vector dimensions. Results show that a convolutional neural network (CNN) attached with MCFA significantly improves the classification performance of CNN, achieving state of the 1 Hereon, we mean to \"fix\" as to \"correct, repair, or alter.\" art performance over multiple data sets.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in intros:\n",
    "    print(i)\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-27T04:31:41.377264Z",
     "start_time": "2022-05-27T04:31:41.342107Z"
    }
   },
   "outputs": [],
   "source": [
    "from text_preprocessing import *\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-27T04:31:41.824735Z",
     "start_time": "2022-05-27T04:31:41.799662Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocessing_pipeline(text):\n",
    "    text = remove_email(text)\n",
    "    text = remove_weblink(text)\n",
    "    text = remove_reference(text)\n",
    "    text = remove_ghost_char(text)\n",
    "    text = remove_brackets(text)\n",
    "    text = remove_extra_spaces(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-27T04:31:42.396082Z",
     "start_time": "2022-05-27T04:31:42.233502Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['deep neural networks have shown great success in various applications such as objection recognition and speech recognition . furthermore, many recent works showed that neural networks can be successfully used in a number of tasks in natural language processing . these include, but are not limited to, language modeling, paraphrase detection and word embedding extraction. in the field of statistical machine translation , deep neural networks have begun to show promising results. summarizes a successful usage of feedforward neural networks in the framework of phrase-based smt system. along this line of research on using neural networks for smt, this paper focuses on a novel neural network architecture that can be used as apart of the conventional phrase-based smt system. the proposed neural network architecture, which we will refer to as an rnn encoder-decoder, consists of two recurrent neural networks that act as an encoder and a decoder pair. the encoder maps a variable-length source sequence to a fixed-length vector, and the decoder maps the vector representation back to a variable-length target sequence. the two networks are trained jointly to maximize the conditional probability of the target sequence given a source sequence. additionally, we propose to use a rather sophisticated hidden unit in order to improve both the memory capacity and the ease of training.the proposed rnn encoder-decoder with a novel hidden unit is empirically evaluated on the task of translating from english to french. we train the model to learn the translation probability of an english phrase to a corresponding french phrase. the model is then used as apart of a standard phrase-based smt system by scoring each phrase pair in the phrase table. the empirical evaluation reveals that this approach of scoring phrase pairs with an rnn encoder-decoder improves the translation performance.we qualitatively analyze the trained rnn encoder-decoder by comparing its phrase scores with those given by the existing translation model. the qualitative analysis shows that the rnn encoder-decoder is better at capturing the linguistic regularities in the phrase table, indirectly explaining the quantitative improvements in the overall translation performance. the further analysis of the model reveals that the rnn encoder-decoder learns a continuous space representation of a phrase that preserves both the semantic and syntactic structure of the phrase. a recurrent neural network is a neural network that consists of a hidden state hand an optional output y which operates on a variablelength sequence x = . at each time step t, the hidden state ht of the rnn is updated bywhere f is a non-linear activation function. f maybe as simple as an elementwise logistic sigmoid function and as complex as along short-term memory unit).an rnn can learn a probability distribution over a sequence by being trained to predict the next symbol in a sequence. in that case, the output at each timestep t is the conditional distribution p. for example, a multinomial distribution can be output using a softmax activation functionfor all possible symbols j = 1, . . . , k, where w j are the rows of a weight matrix w. by combining these probabilities, we can compute the probability of the sequence x usingfrom this learned distribution, it is straightforward to sample anew sequence by iteratively sampling a symbol at each time step.',\n",
       " 'in neural language modelling, a neural network estimates a distribution over sequences of words or characters that belong to a given language. in neural machine translation, the network estimates a distribution over sequences in the target language conditioned on a given sequence in the source language. the network can bethought of as composed of two parts: a source network that encodes the source sequence into a representation and a target network that uses the representation of the source encoder to generate the target sequence.recurrent neural networks are powerful sequence models and are widely used in language modelling), yet they have a potential drawback. rnns have an inherently serial structure that prevents them from being run in parallel along the sequence length during training and evaluation. forward and backward signals in a rnn also need to traverse the full distance of the serial path to reach from one token in the sequence to another. the larger the distance, the harder it is to learn the dependencies between the tokens.a number of neural architectures have been proposed for modelling translation, such as encoder-decoder networks, networks with attentional pooling and twodimensional networks. despite the generally good performance, the proposed models arxiv:1610.10099v2 15 mar 2017 eos eos eos |s| |t| |t|. dynamic unfolding in the bytenet architecture. at each step the decoder is conditioned on the source representation produced by the encoder for that step, or simply on no representation for steps beyond the extended length |t|. the decoding ends when the target network produces an end-of-sequence symbol. either have running time that is super-linear in the length of the source and target sequences, or they process the source sequence into a constant size representation, burdening the model with a memorization step. both of these drawbacks grow more severe as the length of the sequences increases.we present a family of encoder-decoder neural networks that are characterized by two architectural mechanisms aimed to address the drawbacks of the conventional approaches mentioned above. the first mechanism involves the stacking of the decoder on top of the representation of the encoder in a manner that preserves the temporal resolution of the sequences; this is in contrast with architectures that encode the source into a fixed-size representation. the second mechanism is the dynamic unfolding mechanism that allows the network to process in a simple and efficient way source and target sequences of different lengths .the bytenet is the instance within this family of models that uses one-dimensional convolutional neural networks of fixed depth for both the encoder and the decoder). the two cnns use increasing factors of dilation to rapidly grow the receptive fields; a similar technique is also used in. the convolutions in the decoder cnn are masked to prevent the network from seeing future tokens in the target sequence .the network has beneficial computational and learning properties. from a computational perspective, the network has a running time that is linear in the length of the source and target sequences . the computation in the encoder during training and decoding and in the decoder during training can also be run efficiently in parallel along the sequences . from a learning perspective, the representation of the source sequence in the bytenet is resolution preserving; the representation sidesteps the need for memorization and allows for maximal bandwidth between encoder and decoder. in addition, the distance traversed by forward and backward signals between any input and output tokens corresponds to the fixed depth of the networks and is largely independent of the dis-tance between the tokens. dependencies overlarge distances are connected by short paths and can be learnt more easily.we apply the bytenet model to strings of characters for character-level language modelling and character-tocharacter machine translation. we evaluate the decoder network on the hutter prize wikipedia task where it achieves the state-of-the-art performance of 1.31 bits/character. we further evaluate the encoderdecoder network on character-to-character machine translation on the english-to-german wmt benchmark where it achieves a state-of-the-art bleu score of 22.85 and 25.53 on the 2014 and 2015 test sets, respectively. on the character-level machine translation task, bytenet betters a comparable version of gnmt that is a state-of-the-art system. these results show that deep cnns are simple, scalable and effective architectures for challenging linguistic processing tasks.the paper is organized as follows. section 2 lays out the background and some desiderata for neural architectures underlying translation models. section 3 defines the proposed family of architectures and the specific convolutional instance used in the experiments. section 4 analyses bytenet as well as existing neural translation models based on the desiderata set out in section 2. section 5 reports the experiments on language modelling and section 6 reports the experiments on character-to-character machine translation.',\n",
       " 'recurrent neural networks, long short-term memory and gated recurrent neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation. numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures.recurrent models typically factor computation along the symbol positions of the input and output sequences. aligning the positions to steps in computation time, they generate a sequence of hidden states ht , as a function of the previous hidden state h t?1 and the input for position t. this inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. recent work has achieved significant improvements in computational efficiency through factorization tricks and conditional computation, while also improving model performance in case of the latter. the fundamental constraint of sequential computation, however, remains.attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences. in all but a few cases, however, such attention mechanisms are used in conjunction with a recurrent network.in this work we propose the transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. the transformer allows for significantly more parallelization and can reach anew state of the art in translation quality after being trained for as little as twelve hours on eight p100 gpus.',\n",
       " \"neural machine translation has attracted a lot of interest in solving the machine translation problem in recent years. unlike conventional statistical machine translation systems which consist of multiple separately tuned components, nmt models encode the source sequence into continuous representation space and generate the target sequence in an end-to-end fashon. moreover, nmt models can also be easily adapted to other tasks such as dialog systems , question answering systems and image caption generation.in general, there are two types of nmt topologies: the encoder-decoder network and the attention network. the encoder-decoder network represents the source sequence with a fixed dimensional vector and the target sequence is generated from this vector word byword. the attention network uses the representations from all time steps of the input sequence to build a detailed relationship between the target words and the input words. recent results show that the systems based on these models can achieve similar performance to conventional smt systems.however, a single neural model of either of the above types has not been competitive with the best conventional system when evaluated on the wmt'14 english-to-french task. the best bleu score from a single model with six layers is only 31.5 while the conventional method of achieves 37.0.we focus on improving the single model perfor-mance by increasing the model depth. deep topology has been proven to outperform the shallow architecture in computer vision. in the past two years the top positions of the imagenet contest have always been occupied by systems with tensor even hundreds of layers. but in nmt, the biggest depth used successfully is only six. we attribute this problem to the properties of the long short-term memory which is widely used in nmt. in the lstm, there are more non-linear activations than in convolution layers. these activations significantly decrease the magnitude of the gradient in the deep topology, especially when the gradient propagates in recurrent form. there are also many efforts to increase the depth of the lstm such as the work by, where the shortcuts do not avoid the nonlinear and recurrent computation.in this work, we introduce anew type of linear connections for multi-layer recurrent networks. these connections, which are called fast-forward connections, play an essential role in building a deep topology with depth of 16. in addition, we introduce an interleaved bi-directional architecture to stack lstm layers in the encoder. this topology can be used for both the encoder-decoder network and the attention network. on the wmt'14 englishto-french task, this is the deepest nmt topology that has ever been investigated. with our deep attention model, the bleu score can be improved to 37.7 outperforming the shallow model which has six layers by 6.2 bleu points. this is also the first time on this task that a single nmt model achieves state-of-the-art performance and outperforms the best conventional smt system with an improvement of 0.7. even without using the attention mechanism, we can still achieve 36.3 with a single model. after model ensembling and unknown word processing, the bleu score can be further improved to 40.4. when evaluated on the subset of the test corpus without unknown words, our model achieves 41.4. as a reference, previous work showed that oracle rescoring of the 1000-best sequences generated by the smt model can achieve the bleu score of about 45. our models are also validated on the more difficult wmt'14 english-to-german task.\",\n",
       " 'neural machine translation, directly applying a single neural network to transform the source sentence into the target sentence, has now reached impressive performance. the nmt typically consists of two sub neural networks. the encoder network reads and encodes the source sentence into a 1 feng wang is the corresponding author of this paper context vector, and the decoder network generates the target sentence iteratively based on the context vector. nmt can be studied in supervised and unsupervised learning settings. in the supervised setting, bilingual corpora is available for training the nmt model. in the unsupervised setting, we only have two independent monolingual corpora with one for each language and there is no bilingual training example to provide alignment information for the two languages. due to lack of alignment information, the unsupervised nmt is considered more challenging. however, this task is very promising, since the monolingual corpora is usually easy to be collected.motivated by recent success in unsupervised cross-lingual embeddings, the models proposed for unsupervised nmt often assume that a pair of sentences from two different languages can be mapped to a same latent representation in a shared-latent space. following this assumption, use a single encoder and a single decoder for both the source and target languages. the encoder and decoder, acting as a standard auto-encoder , are trained to reconstruct the inputs. and utilize a shared encoder but two independent decoders. with some good performance, they share a glaring defect, i.e., only one encoder is shared by the source and target languages. although the shared encoder is vital for mapping sentences from different languages into the shared-latent space, it is weak in keeping the uniqueness and internal characteristics of each language, such as the style, terminology and sentence structure. since each language has its own characteristics, the source and target languages should be encoded and learned independently. therefore, we conjecture that the shared encoder maybe a factor limit-ing the potential translation performance.in order to address this issue, we extend the encoder-shared model, i.e., the model with one shared encoder, by leveraging two independent encoders with each for one language. similarly, two independent decoders are utilized. for each language, the encoder and its corresponding decoder perform an ae, where the encoder generates the latent representations from the perturbed input sentences and the decoder reconstructs the sentences from the latent representations. to map the latent representations from different languages to a shared-latent space, we propose the weightsharing constraint to the two aes. specifically, we share the weights of the last few layers of two encoders that are responsible for extracting highlevel representations of input sentences. similarly, we share the weights of the first few layers of two decoders. to enforce the shared-latent space, the word embeddings are used as a reinforced encoding component in our encoders. for cross-language translation, we utilize the backtranslation following. additionally, two different generative adversarial networks , namely the local and global gan, are proposed to further improve the cross-language translation. we utilize the local gan to constrain the source and target latent representations to have the same distribution, whereby the encoder tries to fool a local discriminator which is simultaneously trained to distinguish the language of a given latent representation. we apply the global gan to finetune the corresponding generator, i.e., the composition of the encoder and decoder of the other language, where a global discriminator is leveraged to guide the training of the generator by assessing how far the generated sentence is from the true data distribution 1 . in summary, we mainly make the following contributions: we propose the weight-sharing constraint to unsupervised nmt, enabling the model to utilize an independent encoder for each language. to enforce the shared-latent space, we also propose the embedding-reinforced encoders and two different gans for our model. we conduct extensive experiments on the code that we utilized to train and evaluate our models can be found at link english-german, english-french and chinese-to-english translation tasks. experimental results show that the proposed approach consistently achieves great success. last but not least, we introduce the directional self-attention to model temporal order information for the proposed model. experimental results reveal that it deserves more efforts for researchers to investigate the temporal order information within self-attention layers of nmt.',\n",
       " 'neural machine translation is a rapidly changing research area. since 2016 when nmt systems first showed to achieve significantly better results than statistical machine translation systems, the dominant neural network architectures for nmt have changed on a yearly basis. the state-of-the-art in 2016 were shallow attention-based recurrent neural networks with gated recurrent units in recurrent layers. in 2017, multiplicative long short-term memory units and deep gru models were introduced in nmt. the same year, selfattentional models were introduced. consequently, in 2018, most of the top scoring systems in the shared task on news translation of the third conference on machine translation were trained using transformer models 1 . however, it is already evident that the state-of-the-art architectures will 1 all 14 of the best automatically scored systems according to the information provided by participants in the official submission portal link were indicated as being based on transformer models. be pushed even further in 2018. for instance, have recently proposed rnmt+ models that combine deep lstm-based models with multi-head attention and showed that the models outperform transformer models.in wmt 2017, tilde participated with mlstm-based nmt systems. in this paper, we compare the mlstmbased models with transformer models for english-estonian and estonian-english and we show that the state-of-the-art of wmt 2017 is well behind the new models. therefore, for wmt 2018, tilde submitted nmt systems that were trained using transformer models.the paper is further structured as follows: section 2 provides an overview of systems submitted for the wmt 2018 shared task on news translation, section 3 describes the data used to train the nmt systems and the data pre-processing workflows, section 4 describes all nmt systems trained and experiments on handling of named entities and combination of systems, section 5 provides automatic evaluation results, and section 6 concludes the paper.',\n",
       " 'word embeddings, which are distributed and continuous vector representations for word tokens, have been one of the basic building blocks for many neural network-based models used in natural language processing tasks, such as language modeling, text classification and machine translation. different from classic one-hot representation, the learned word embeddings contain semantic information which can measure the semantic similarity between words, and can also be transferred into other learning tasks.in deep learning approaches for nlp tasks, word embeddings act as the inputs of the neural network and are usually trained together with neural network parameters. as the inputs of the neural network, word embeddings carryall the information of words that will be further processed by the network, and the quality of embeddings is critical and highly impacts the final performance of the learning task. unfortunately, we find the word embeddings learned by many deep learning approaches are far from perfect. as shown in and 1, in the embedding space learned by word2vec model, the nearest neighbors of word \"peking\" includes \"quickest\", \"multicellular\", and \"epigenetic\", which are not semantically similar, while semantically related words such as \"beijing\" and \"china\" are far from it. similar phenomena are observed from the word embeddings learned from translation tasks.with a careful study, we find a more general problem which is rooted in low-frequency words in the text corpus. without any confusion, we also call high-frequency words as popular words and call low-frequency words as rare words. as is well known, the frequency distribution of words roughly follows a simple mathematical form known as zipf\\'s law. when the size of a text corpus grows, the frequency of rare words is much smaller than popular words while the number of unique rare words is much larger than popular words. interestingly, the learned embeddings of rare words and popular words behave differently. in the embedding space, a popular word usually has semantically related neighbors, while a rare word usually does not. moreover, the nearest neighbors of more than 85% rare words are rare words. word embeddings encode frequency information. as shown in and 1, the embeddings of rare words and popular words actually lie in different subregions of the space. such a phenomenon is also observed in.we argue that the different behaviors of the embeddings of popular words and rare words are problematic. first, such embeddings will affect the semantic understanding of words. we observe more than half of the rare words are nouns or variants of popular words. those rare words should have similar meanings or share the same topics with popular words. second, the neighbors of a large number of rare words are semantically unrelated rare words. to some extent, those word embeddings encode more frequency information than semantic information which is not good from the view of semantic understanding. it will consequently limit the performance of down-stream tasks using the embeddings. for example, in text classification, it cannot be well guaranteed that the label of a sentence does not change when you replace one popular/rare word in the sentence by its rare/popular alternatives.to address this problem, in this paper, we propose an adversarial training method to learn frequency-agnostic word embedding . for a given nlp task, in addition to minimize the task-specific loss by optimizing the task-specific parameters together with word embeddings, we introduce another discriminator, which takes a word embedding as input and classifies whether it is a popular/rare word. the discriminator optimizes its parameters to maximize its classification accuracy, while word embeddings are optimized towards a low task-dependent loss as well as fooling the discriminator to mis-classify the popular and rare words. when the whole training process converges and the system achieves an equilibrium, the discriminator cannot well differentiate popular words from rare words. consequently, rare words lie in the same region as and are mixed with popular words in the embedding space. then frage will catch better semantic information and help the task-specific model to perform better.we conduct experiments on four types of nlp tasks, including three word similarity tasks, two language modeling tasks, three sentiment classification tasks and two machine translation tasks to test our method. in all tasks, frage outperforms the baselines. specifically, in language modeling and machine translation, we achieve better performance than the state-of-the-art results on ptb, wt2 and wmt14 english-german datasets.',\n",
       " 'due to the explosive growth of data, subset selection methods are increasingly popular fora wide range of machine learning and computer vision applications. this kind of methods offer the potential to select a few highly representative samples or exemplars to describe the entire dataset. by analyzing a few, we can roughly know all. such case is very important to summarize and visualize huge datasets of texts, images and videos etc.. besides, by only using the selected exemplars for succeeding tasks, the cost of memories and computational time will be greatly reduced. additionally, as outliers are generally less representative, the side effect of outliers will be reduced, thus boosting the performance of subsequent applications.there have been several subset selection methods. the most intuitional method is to randomly select a fixed number of samples. although highly efficient, there is no guarantee for an effective selection. for the other methods, depending on the mechanism of representative exemplars, there are mainly three categories of selection methods. one category data size selection time',\n",
       " 'named entity recognition is a challenging learning problem. one the one hand, inmost languages and domains, there is only a very small amount of supervised training data available. on the other, there are few constraints on the kinds of words that can be names, so generalizing from this small sample of data is difficult. as a result, carefully constructed orthographic features and language-specific knowledge resources, such as gazetteers, are widely used for solving this task. unfortunately, languagespecific resources and features are costly to develop in new languages and new domains, making ner a challenge to adapt. unsupervised learning from unannotated corpora offers an alternative strategy for obtaining better generalization from small amounts of supervision. however, even systems that have relied extensively on unsupervised features have used these to augment, rather than replace, hand-engineered features and specialized knowledge resources .in this paper, we present neural architectures for ner that use no language-specific resources or features beyond a small amount of supervised training data and unlabeled corpora. our models are designed to capture two intuitions. first, since names often consist of multiple tokens, reasoning jointly over tagging decisions for each token is important. we compare two models here, a bidirectional lstm with a sequential conditional random layer above it , and anew model that constructs and labels chunks of input sentences using an algorithm inspired by transition-based parsing with states represented by stack lstms . second, token-level evidence for \"being a name\" includes both orthographic evidence and distributional evidence . to capture orthographic sensitivity, we use character-based word representation model to capture distributional sensitivity, we combine these representations with distributional representations. our word representations combine both of these, and dropout training is used to encourage the model to learn to trust both sources of evidence .experiments in english, dutch, german, and spanish show that we are able to obtain state-of-the-art ner performance with the lstm-crf model in dutch, german, and spanish, and very near the state-of-the-art in english without any hand-engineered features or gazetteers . the transition-based algorithm likewise surpasses the best previously published results in several languages, although it performs less well than the lstm-crf model.',\n",
       " \"in order to democratize large-scale nlp and information extraction while minimizing our environmental footprint, we require fast, resource-efficient methods for sequence tagging tasks such as part-of-speech tagging and named entity recognition . speed is not sufficient of course: they must also be expressive enough to tolerate the tremendous lexical variation in input data.the massively parallel computation facilitated by gpu hardware has led to a surge of successful neural network architectures for sequence labeling. while these models are expressive and accurate, they fail to fully exploit the parallelism opportunities of a gpu, and thus their speed is limited. specifically, they employ either recurrent neural networks for feature extraction, or viterbi inference in a structured output model, both of which require sequential computation across the length of the input.instead, parallelized runtime independent of the length of the sequence saves time and energy costs, maximizing gpu resource usage and minimizing the amount of time it takes to train and evaluate models. convolutional neural networks provide exactly this property. rather than composing representations incrementally over each token in a sequence, they apply filters in parallel across the entire sequence at once. their computational cost grows with the number of layers, but not the input size, up to the memory and threading limitations of the hardware. this provides, for example, audio generation models that can be trained in parallel + 1. the number of layers required to incorporate the entire input context grows linearly with the length of the sequence. to avoid this scaling, one could pool representations across the sequence, but this is not appropriate for sequence labeling, since it reduces the output resolution of the representation.in response, this paper presents an application of dilated convolutions for sequence labeling). for dilated convolutions, the effective input width can grow exponentially with the depth, with no loss in resolution at each layer and with a modest number of parameters to estimate. like typical cnn layers, dilated convolutions operate on a sliding window of context over the sequence, but unlike conventional convolutions, the context need not be consecutive; the dilated window skips over every dilation width d inputs. by stacking layers of dilated convolutions of exponentially increasing dilation width, we can expand the size of the effective input width to cover the entire length of most sequences using only a few layers: the size of the effective input width fora token at layer l is now given by 2 l+1 ?1. more concretely, just four stacked dilated convolutions of width 3 produces token representations with an effective input width of 31 tokens -longer than the average sentence length in the penn treebank.our overall iterated dilated cnn architecture repeatedly applies the same block of dilated convolutions to token-wise representations. this parameter sharing prevents overfitting and also provides opportunities to inject supervision on intermediate activations of the network. similar to models that use logits produced by an rnn, the id-cnn provides two methods for performing prediction: we can predict each token's label independently, or by running viterbi inference in a chain structured graphical model.in experiments on conll 2003 and ontonotes 1 what we call effective input width here is known as the receptive field in the vision literature, drawing an analogy to the visual receptive field of a neuron in the retina.: a dilated cnn block with maximum dilation width 4 and filter width 3. neurons contributing to a single highlighted neuron in the last layer are also highlighted. 5.0 english ner, we demonstrate significant speed gains of our id-cnns over various recurrent models, while maintaining similar f1 performance. when performing prediction using independent classification, the id-cnn consistently outperforms a bidirectional lstm , and performs on par with inference in a crf with logits from a bi-lstm . as an extractor of per-token logits fora crf, our model out-performs the bi-lstm-crf. we also apply id-cnns to entire documents, where independent token classification is as accurate as the bi-lstm-crf while decoding almost 8 faster. the clear accuracy gains resulting from incorporating broader context suggest that these models could similarly benefit many other contextsensitive nlp tasks which have until now been limited by the computational complexity of existing context-rich models. 2 2 background\",\n",
       " 'due to their simplicity and efficacy, pre-trained word embedding have become ubiquitous in nlp systems. many prior studies have shown that they capture useful semantic and syntactic information and including them in nlp systems has been shown to be enormously helpful fora variety of downstream tasks.however, in many nlp tasks it is essential to represent not just the meaning of a word, but also the word in context. for example, in the two phrases \"a central bank spokesman\" and \"the central african republic\", the word \\'central\\' is used as part of both an organization and location. accordingly, current state of the art sequence tagging models typically include a bidirectional re-current neural network that encodes token sequences into a context sensitive representation before making token specific predictions.although the token representation is initialized with pre-trained embeddings, the parameters of the bidirectional rnn are typically learned only on labeled data. previous work has explored methods for jointly learning the bidirectional rnn with supplemental labeled data from other tasks , pre-trained on a large, unlabeled corpus to compute an encoding of the context at each position in the sequence and use it in the supervised sequence tagging model. since the lm embeddings are used to compute the probability of future words in a neural lm, they are likely to encode both the semantic and syntactic roles of words in context.our main contribution is to show that the context sensitive representation captured in the lm embeddings is useful in the supervised sequence tagging setting. when we include the lm embeddings in our system overall performance increases from 90.87% to 91.93% f 1 for the conll 2003 ner task, a more then 1% absolute f1 increase, and a substantial improvement over the previous state of the art. we also establish anew state of the art result for the conll 2000 chunking task.as a secondary contribution, we show that using both forward and backward lm embeddings boosts performance over a forward only lm. we also demonstrate that domain specific pre-training is not necessary by applying a lm trained in the news domain to scientific papers. the main components in our language-modelaugmented sequence tagger are illustrated in. after pre-training word embeddings and a neural lm on large, unlabeled corpora , we extract the word and lm embeddings for every token in a given input sequencestep 2) and use them in the supervised sequence tagging model .',\n",
       " 'pre-trained word representations area key component in many neural language understanding models. however, learning high quality representations can be challenging. they should ideally model both complex characteristics of word use , and how these uses vary across linguistic contexts . in this paper, we introduce anew type of deep contextualized word representation that directly addresses both challenges, can be easily integrated into existing models, and significantly improves the state of the art in every considered case across a range of challenging language understanding problems.our representations differ from traditional word type embeddings in that each token is assigned a representation that is a function of the entire input sentence. we use vectors derived from a bidirectional lstm that is trained with a coupled lan-guage model objective on a large text corpus. for this reason, we call them elmo representations. unlike previous approaches for learning contextualized word vectors, elmo representations are deep, in the sense that they area function of all of the internal layers of the bilm. more specifically, we learn a linear combination of the vectors stacked above each input word for each end task, which markedly improves performance over just using the top lstm layer.combining the internal states in this manner allows for very rich word representations. using intrinsic evaluations, we show that the higher-level lstm states capture context-dependent aspects of word meaning while lowerlevel states model aspects of syntax . simultaneously exposing all of these signals is highly beneficial, allowing the learned models select the types of semi-supervision that are most useful for each end task.extensive experiments demonstrate that elmo representations work extremely well in practice. we first show that they can be easily added to existing models for six diverse and challenging language understanding problems, including textual entailment, question answering and sentiment analysis. the addition of elmo representations alone significantly improves the state of the art in every case, including up to 20% relative error reductions. for tasks where direct comparisons are possible, elmo outperforms cove, which computes contextualized representations using a neural machine translation encoder. finally, an analysis of both elmo and cove reveals that deep representations outperform arxiv:1802.05365v2 22 mar 2018 those derived from just the top layer of an lstm. our trained models and code are publicly available, and we expect that elmo will provide similar gains for many other nlp problems. 1 2 related work due to their ability to capture syntactic and semantic information of words from large scale unlabeled text, pretrained word vectors area standard component of most state-ofthe-art nlp architectures, including for question answering, textual entailment and semantic role labeling. however, these approaches for learning word vectors only allow a single contextindependent representation for each word.previously proposed methods overcome some of the shortcomings of traditional word vectors by either enriching them with subword information uses a bidirectional long short term memory to encode the context around a pivot word. other approaches for learning contextual embeddings include the pivot word itself in the representation and are computed with the encoder of either a supervised neural machine translation system or an unsupervised language model. both of these approaches benefit from large datasets, although the mt approach is limited by the size of parallel corpora. in this paper, we take full advantage of access to plentiful monolingual data, and train our bilm on a corpus with approximately 30 million sentences. we also generalize these approaches to deep contextual representations, which we show work well across abroad range of diverse nlp tasks. 1 link previous work has also shown that different layers of deep birnns encode different types of information. for example, introducing multi-task syntactic supervision at the lower levels of a deep lstm can improve overall performance of higher level tasks such as dependency parsing or ccg super tagging. in an rnn-based encoder-decoder machine translation system, showed that the representations learned at the first layer in a 2layer lstm encoder are better at predicting pos tags then second layer. finally, the top layer of an lstm for encoding word context has been shown to learn representations of word sense. we show that similar signals are also induced by the modified language model objective of our elmo representations, and it can be very beneficial to learn models for downstream tasks that mix these different types of semi-supervision. and pretrain encoder-decoder pairs using language models and sequence autoencoders and then fine tune with task specific supervision. in contrast, after pretraining the bilm with unlabeled data, we fix the weights and add additional taskspecific model capacity, allowing us to leverage large, rich and universal bilm representations for cases where downstream training data size dictates a smaller supervised model.',\n",
       " 'neural models have become the dominant approach in the nlp literature. compared to handcrafted indicator features, neural sentence representations are less sparse, and more flexible in encoding intricate syntactic and semantic information. among various neural networks for encoding sentences, bi-directional lstms have been a dominant method, giving state-of-the-art results in language modelling, machine translation, syntactic parsing and question answering.despite their success, bilstms have been shown to suffer several limitations. for example, their inherently sequential nature endows computation non-parallel within the same sentence, which can lead to a computational bottleneck, hindering their use in the in- dustry. in addition, local ngrams, which have been shown a highly useful source of contextual information for nlp, are not explicitly modelled. finally, sequential information flow leads to relatively weaker power in capturing longrange dependencies, which results in lower performance in encoding longer sentences.we investigate an alternative recurrent neural network structure for addressing these issues. as shown in, the main idea is to model the hidden states of all words simultaneously at each recurrent step, rather than one word at a time. in particular, we view the whole sentence as a single state, which consists of sub-states for individual words and an overall sentence-level state. to capture local and non-local contexts, states are updated recurrently by exchanging information between each other. consequently, we refer to our model as sentence-state lstm, or s-lstm in short. empirically, s-lstm can give effective sentence encoding after 3 -6 recurrent steps. in contrast, the number of recurrent steps necessary for bilstm scales with the size of the sentence.at each recurrent step, information exchange is conducted between consecutive words in the sentence, and between the sentence-level state and each word. in particular, each word receives information from its predecessor and successor simultaneously. from an initial state without information exchange, each word-level state can obtain 3-gram, 5-gram and 7-gram information after 1, 2 and 3 recurrent steps, respectively. being connected with every word, the sentence-level state vector serves to exchange non-local information with each word. in addition, it can also be used as a global sentence-level representation for classification tasks.results on both classification and sequence labelling show that s-lstm gives better accuracies compared to bilstm using the same number of parameters, while being faster. we release our code and models at link leuchine/s-lstm, which include all baselines and the final model.',\n",
       " 'named-entity recognition is the task of identifying textual mentions and classifying them into a predefined set of types. various approaches have been proposed to tackle the task, from hand-crafted feature-based machine learning models like conditional random fields and perceptron, to deep neural models.word representations, also known as word embeddings, area key element for multiple nlp tasks including ner. due to the small amount of named-entity annotated data, embeddings are used to extend, rather than replace, hand-crafted features in order to obtain state-of-the-art performance. recent studies have explored methods for supplying deep sequential taggers with complementary features to standard embeddings. and tested special embeddings extracted from a neural language model trained on a large corpus. lm embeddings capture context-dependent aspects of word meaning using future and previous context words. when this information is added to standard features, it leads to significant improvements in ner. also, showed that external knowledge resources are crucial to ner performance. gazetteer features encode the presence of word n-grams in predefined lists of nes.in this work, we discuss some of the limitations of gazetteer features and propose an alternative lexical representation which is trained offline and that can be added to any neural ner system. ina nutshell, we embed words and entity types into a joint vector space by leveraging wifine, a ressource which automatically annotates mentions in wikipedia with 120 entity types. from this vector space, we compute for each word a 120-dimensional vector, where each dimension encodes the similarity of the word with an entity type. we call this vector an ls representation, for lexical similarity. when included in a vanilla lstm-crf ner model, ls representations lead to significant gains. we establish anew state-of-the-art f1 score of 87.95 on ontonotes 5.0, while matching state-of-the-art performance on the over-studied in the rest of this paper, we motivate our work in section 2. we describe how we compute ls vectors in section 3. we present our system in section 4 and report results in section 5. in section 6, we discuss related works before concluding in section 7.',\n",
       " \"there has been an increasing interest in named entity recognition or more generally recognizing entity mentions 2) that the nested hierarchical structure of entity mentions should betaken into account to better facilitate downstream tasks like question answering, relation extraction, event extraction, and coreference resolution. practically, the mentions with nested structures frequently exist in news and biomedical documents. for example in traditional sequence labeling models such as conditional random fields do not allow hierarchical structures between segments, making them incapable to handle such problems. presented a chart-based parsing approach where each sentence with nested mentions is mapped to a rooted constituent tree. the issue of using a chart-based parser is its cubic time complexity in the number of words in the sentence.to achieve a scalable and effective solution for recognizing nested mentions, we design a transition-based system which is inspired by the recent success of employing transition-based methods for constituent parsing) and named entity recognition, especially when they are paired with neural networks. generally, each sentence with nested mentions is mapped to a forest where each outermost mention forms a tree consisting of its inner mentions. then our transition-based system learns to construct this forest through a sequence of shift-reduce actions. shows an example of such a forest. in contrast, the tree structure by further uses a root node to connect all tree elements. our forest representation eliminates the root node so that the number of actions required to construct it can be reduced significantly.following, we employ stack-lstm to represent the system's state, which consists of the states of input, stack and action history, in a continuous space incrementally. the processed nested mentions in the stack are encoded with recursive neural networks where composition functions are used to capture dependencies between nested mentions. based on the observation that letter-level patterns such as capitalization and prefix can be beneficial in detecting mentions, we incorporate a characterlevel lstm to capture such morphological information. meanwhile, this character-level component can also help deal with the out-of-vocabulary problem of neural models. we conduct experiments in three standard datasets. our system achieves the state-of-the-art performance on ace datasets and comparable performance in genia dataset.\",\n",
       " 'language model pre-training has been shown to be effective for improving many natural language processing tasks. these include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.there are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. the feature-based approach, such as elmo, uses task-specific architectures that include the pre-trained representations as additional features. the fine-tuning approach, such as the generative pre-trained transformer , introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters. the two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.we argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches. the major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. for example, in openai gpt, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the transformer. such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.in this paper, we improve the fine-tuning based approaches by proposing bert: bidirectional encoder representations from transformers. bert alleviates the previously mentioned unidirectionality constraint by using a \"masked language model\" pre-training objective, inspired by the cloze task. the masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked word based only on its context. unlike left-toright language model pre-training, the mlm objective enables the representation to fuse the left and the right context, which allows us to pretrain a deep bidirectional transformer. in addition to the masked language model, we also use a \"next sentence prediction\" task that jointly pretrains text-pair representations. the contributions of our paper are as follows: we demonstrate the importance of bidirectional pre-training for language representations. unlike, which uses unidirectional language models for pre-training, bert uses masked language models to enable pretrained deep bidirectional representations. this is also in contrast to, which uses a shallow concatenation of independently trained left-to-right and right-to-left lms. we show that pre-trained representations reduce the need for many heavily-engineered taskspecific architectures. bert is the first finetuning based representation model that achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks, outperforming many task-specific architectures. bert advances the state of the art for eleven nlp tasks. the code and pre-trained models are available at link google-research/bert.',\n",
       " 'the volume of biomedical literature continues to rapidly increase. on average, more than 3000 new articles are published everyday in peer-reviewed journals, excluding pre-prints and technical reports such as clinical trial reports in various archives. pubmed alone has a total of 29m articles as of january 2019. reports containing valuable information about new discoveries and new insights are continuously added to the already overwhelming amount of literature. consequently, there is increasingly more demand for accurate biomedical text mining tools for extracting information from the literature.recent progress of biomedical text mining models was made possible by the advancements of deep learning techniques used in natural language processing . for instance, long short-term memory and conditional random field have greatly improved performance in biomedical named entity recognition over the last few years. other deep learning based models have made improvements in biomedical text mining tasks such as relation extraction and question answering .however, directly applying state-of-the-art nlp methodologies to biomedical text mining has limitations. first, as recent word representation models such as word2vec, elmo and bert are trained and tested mainly on datasets containing general domain texts , it is difficult to estimate their performance on datasets containing biomedical texts. also, the word distributions of general and biomedical corpora are quite different, which can often be a problem for biomedical text mining models. as a result, recent models in biomedical text mining rely largely on adapted versions of word representations.in this study, we hypothesize that current state-of-the-art word representation models such as bert need to be trained on biomedical corpora to be effective in biomedical text mining tasks. previously, word2vec, which is one of the most widely known context independent word representation models, was trained on biomedical corpora which contain terms and expressions that are usually not included in a general domain corpus. while elmo and bert have proven the effectiveness of contextualized word representations, they cannot obtain high performance on biomedical corpora because they are pre-trained on only general domain corpora. as bert achieves very strong results on various nlp tasks while using almost the same structure across the tasks, adapting bert for the biomedical domain could potentially benefit numerous biomedical nlp researches.',\n",
       " \"this paper addresses the challenging problem of open-domain question answering, which consists of building systems able to answer questions from any domain. any advance on this difficult topic would bring a huge leap forward in building new ways of accessing knowledge. an important development in this area has been the creation of large-scale knowledge bases , such as freebase and dbpedia which store huge amounts of general-purpose information. they are organized as databases of triples connecting pairs of entities by various relationships and of the form . question answering is then defined as the task of retrieving the correct entity or set of entities from a kb given a query expressed as a question in natural language.the use of kbs simplifies the problem by separating the issue of collecting and organizing information from the one of searching through it . however, open question answering remains challenging because of the scale of these kbs and of the difficulty for machines to interpret natural language. recent progress has been made by tackling this problem with semantic parsers. these methods convert questions into logical forms or database queries which are then subsequently used to query kbs for answers. even if such systems have shown the ability to handle large-scale kbs, they require practitioners to hand-craft lexicons, grammars, and kb schema for the parsing to be effective. this nonnegligible human intervention might not be generic enough to conveniently scale up to new databases with other schema, broader vocabularies or other languages than english.in this paper, we instead take the approach of converting questions to vectorial representations which require no pre-defined grammars or lexicons and can query any kb independent of its schema. following, we focus on answering simple factual questions on abroad range of topics, more specifically, those for which single kb triples stand for both the question and an answer . for example, stands for what is parrotfish's habitat? and southern-water.e and for what is the main language of hong-kong? and cantonese.e. in this task, the main difficulties come from lexical variability rather than from complex syntax, having multiple answers per question, and the absence of a supervised training signal.our approach is based on learning low-dimensional vector embeddings of words and of kb triples so that representations of questions and corresponding answers end up being similar in the embedding space. unfortunately, we do not have access to any human labeled supervision for this task. in order to avoid transferring the cost of manual intervention to the one of labeling large amounts of data, we make use of weak supervision. we show empirically that our model is able to take advantage of noisy and indirect supervision by automatically generating questions from kb triples and treating this as training data; and supplementing this with a data set of questions collaboratively marked as paraphrases but with no associated answers. we end up learning meaningful vectorial representations for questions involving up to 800k words and for triples of an mostly automatically created kb with 2.4m entities and 600k relationships. our method strongly outperforms previous results on the wikianswers+reverb evaluation data set introduced by. even if the embeddings obtained after training are of good quality, the scale of the optimization problem makes it hard to control and to lead to convergence. thus, we propose a method to fine-tune embedding-based models by carefully optimizing a matrix parameterizing the similarity used in the embedding space, leading to a consistent improvement in performance.the rest of the paper is organized as follows. section 2 discusses some previous work and section 3 introduces the problem of open question answering. then, section 4 presents our model and section 5 our experimental results.\",\n",
       " 'matching two potentially heterogenous language objects is central to many natural language applications. it generalizes the conventional notion of similarity or relevance , since it aims to model the correspondence between \"linguistic objects\" of different nature at different levels of abstractions. examples include top-k re-ranking in machine translation and dialogue . natural language sentences have complicated structures, both sequential and hierarchical, that are essential for understanding them. a successful sentence-matching algorithm therefore needs to capture not only the internal structures of sentences but also the rich patterns in their interactions. towards this end, we propose deep neural network models, which adapt the convolutional strategy to natural language. to further explore the relation between representing sentences and matching them, we devise a novel model that can naturally host both the hierarchical composition for sentences and the simple-to-comprehensive fusion of matching patterns with the same convolutional architecture. our model is generic, requiring no prior knowledge of natural language and putting essentially no constraints on the matching tasks. this is part of our continuing effort 1 in understanding natural language objects and the matching between them.our main contributions can be summarized as follows. first, we devise novel deep convolutional network architectures that can naturally combine 1) the hierarchical sentence modeling through layer-by-layer composition and pooling, and 2) the capturing of the rich matching patterns at different levels of abstraction; second, we perform extensive empirical study on tasks with different scales and characteristics, and demonstrate the superior power of the proposed architectures over competitor methods.roadmap we start by introducing a convolution network in section 2 as the basic architecture for sentence modeling, and how it is related to existing sentence models. based on that, in section 3, we propose two architectures for sentence matching, with a detailed discussion of their relation. in section 4, we briefly discuss the learning of the proposed architectures. then in section 5, we report our empirical study, followed by a brief discussion of related work in section 6.',\n",
       " 'open-domain question answering systems aim at providing the exact answer to questions formulated in natural language, without restriction of domain. while there is along history of qa systems that search for textual documents or on the web and extract answers from them , recent progress has been made with the release of large knowledge bases such as freebase, which contain consolidated knowledge stored as atomic facts, and extracted from different sources, such as free text, tables in webpages or collaborative input. existing approaches for qa from kbs use learnable components to either transform the question into a structured kb query or learn to embed questions and facts in a low dimensional vector space and retrieve the answer by computing similarities in this embedding space. however, while most recent efforts have focused on designing systems with higher reasoning capabilities, that could jointly retrieve and use multiple facts to answer, the simpler problem of answering questions that refer to a single fact of the kb, which we call simple question answering in this paper, is still far from solved.hence, existing benchmarks are small; they mostly cover the head of the distributions of facts, and are restricted in their question types and their syntactic and lexical variations. as such, it is still unknown how much the existing systems perform outside the range of the specific question templates of a few, small benchmark datasets, and it is also unknown whether learning on a single dataset transfers well on other ones, and whether such systems can learn from different training sources, which we believe is necessary to capture the whole range of possible questions.besides, the actual need for reasoning, i.e. constructing the answer from more than a single fact from the kb, depends on the actual structure of the kb. as we shall see, for instance, a simple preprocessing of freebase tremendously increases the coverage of simple qa in terms of possible questions that can be answered with a single fact, including list questions that expect more than a single answer. in fact, the task of simple qa itself might already cover a wide range of practical usages, if the kb is properly organized. this paper presents two contributions. first, as an effort to study the coverage of existing systems and the possibility to train jointly on different data sources via multitasking, we collected the first large-scale dataset of questions and answers based on a kb, called simplequestions. this dataset, which is presented in section 2, contains more than 100k questions written by human anno-what american cartoonist is the creator of andy lippincott? which forest is fires creek in? what is an active ingredient in childrens earache relief ? tators and associated to freebase facts, while the largest existing benchmark, webquestions, contains less than 6k questions created automatically using the google suggest api.second, in sections 3 and 4, we present an embedding-based qa system developed under the framework of memory networks . memory networks are learning systems centered around a memory component that can be read and written to, with a particular focus on cases where the relationship between the input and response languages and the storage language is performed by embedding all of them in the same vector space. the setting of the simple qa corresponds to the elementary operation of performing a single lookup in the memory. while our model bares similarity with previous embedding models for qa, using the framework of memnns opens the perspective to more involved inference schemes in future work, since memnns were shown to perform well on complex reasoning toy qa tasks. we discuss related work in section 5.we report experimental results in section 6, where we show that our model achieves excellent results on the benchmark webquestions. we also show that it can learn from two different qa datasets to improve its performance on both. we also present the first successful application of transfer learning for qa. using the reverb kb and qa datasets, we show that reverb facts can be added to the memory and used to answer without retraining, and that memnns achieve better results than some systems designed on this dataset.',\n",
       " \"humans learn in a variety of ways-by communication with each other, and by study, the reading of text. comprehension of unstructured text by machines, at a near-human level, is a major goal for natural language processing. it has garnered significant attention from the machine learning research community in recent years.machine comprehension is evaluated by posing a set of questions based on a text passage . such tests are objectively gradable and can be used to assess a range of abilities, from basic understanding to causal reasoning to inference. given a text passage and a question about its content, a system is tested on its ability to determine the correct answer. in this work, we focus on mctest, a complex but data-limited comprehension benchmark, whose multiple-choice questions require not only extraction but also inference and limited reasoning. inference and reasoning are important human skills that apply broadly, beyond language.we present a parallel-hierarchical approach to machine comprehension designed to work well in a data-limited setting. there are many use-cases in which comprehension over limited data would be handy: for example, user manuals, internal documentation, legal contracts, and soon. moreover, work towards more efficient learning from any quantity of data is important in its own right, for bringing machines more inline with the way humans learn. typically, artificial neural networks require numerous parameters to capture complex patterns, and the more parameters, the more training data is required to tune them. likewise, deep models learn to extract their own features, but this is a data-intensive process. our model learns to comprehend at a high level even when data is sparse.the key to our model is that it compares the question and answer candidates to the text using several distinct perspectives. we refer to a question combined with one of its answer candidates as a hypothesis . the semantic perspective compares the hypothesis to sentences in the text viewed as single, self-contained thoughts; these are represented using a sum and transformation of word embedding vectors, similarly to in . the word-by-word perspective focuses on similarity matches between individual words from hypothesis and text, at various scales. as in the semantic perspective, we consider matches over complete sentences. we also use a sliding window acting on a subsentential scale , which implicitly considers the linear distance between matched words. finally, this word-level sliding window operates on two different views of text sentences: the sequential view, where words appear in their natural order, and the dependency view, where words are reordered based on a linearization of the sentence's dependency graph. words are represented throughout by embedding vectors. these distinct perspectives naturally form a hierarchy that we depict in. language is hierarchical, so it makes sense that comprehension relies on hierarchical levels of understanding.the perspectives of our model can be considered a type of feature. however, they are implemented by parametric differentiable functions. this is in contrast to most previous efforts on mctest, whose numerous hand-engineered features cannot be trained. our model, significantly, can be trained end-to-end with backpropagation. to facilitate learning with limited data, we also develop a unique training scheme. we initialize the model's neural networks to perform specific heuristic functions that yield decent performance on the dataset. thus, the training scheme gives the model a safe, reasonable baseline from which to start learning. we call this technique training wheels.computational models that comprehend have developed contemporaneously in several research groups. models designed specifically for mctest include those of, and more recently,, and. in experiments, our parallel-hierarchical model achieves state-of-the-art accuracy on mctest, outperforming these existing methods.below we describe related work, the mathematical details of our model, and our experiments, then analyze our results.\",\n",
       " 'recently, the idea of training machine comprehension models that can read, understand, and answer questions about a text has come closer to reality principally through two factors. the first is the advent of deep learning techniques, which allow manipulation of natural language beyond its surface forms and generalize beyond relatively small amounts of labeled data. the second factor is the formulation of standard machine comprehension benchmarks based on cloze-style queries, which permit fast integration loops between model conception and experimental evaluation.cloze-style queries are created by deleting a particular word in a natural-language statement. the task is to guess which word was deleted. ina pragmatic approach, recent work formed such questions by extracting a sentence from a larger document. in contrast to considering a stand-alone statement, the system is now required to handle a larger amount of information that may possibly influence the prediction of the missing word. such contextual dependencies may also be injected by removing a word from a short human-crafted summary of a larger body of text. the abstractive nature of the summary is likely to demand a higher level of comprehension of the original text. in both cases, the machine comprehension system is presented with an ablated query and the document to which the original query refers. the missing word is assumed to appear in the document. encouraged by the recent success of deep learning attention architectures, we propose a novel neural attention-based inference model designed to perform machine reading comprehension tasks. the model first reads the document and the query using a recurrent neural network. then, it deploys an iterative inference process to uncover the inferential links that exist between the missing query word, the query, and the document. this phase involves a novel alternating attention mechanism; it first attends to some parts of the query, then finds their corresponding matches by attending to the document. the result of this alternating search is fed back into the iterative inference process to seed the next search step. this permits our model to reason about different parts of the query in a sequential way, based on the information that has been gathered previously from the document. after a fixed number of iterations, the model uses a summary of its inference process to predict the answer. this paper makes the following contributions. we present a novel iterative, alternating attention mechanism that, unlike existing models, does not compress the query to a single representation, but instead alternates its attention between the query and the document to obtain a fine-grained query representation within a fixed computation time. our architecture tightly integrates previous ideas related to bidirectional readers and iterative attention processes. it obtains state-of-theart results on two machine comprehension datasets and shows promise for application to abroad range of natural language processing tasks.',\n",
       " 'extracting semantic relations between entities in text is an important and well-studied task in information extraction and natural language processing . traditional systems treat this task as a pipeline of two separated tasks, i.e., named entity recognition ) and relation extraction, but recent studies show that end-to-end modeling of entity and relation is important for high performance since relations interact closely with entity information. for instance, to learn that toefting and bolton have an organization-affiliation relation in the sentence toefting transferred to bolton, the entity information that toefting and bolton are person and organization entities is important. extraction of these entities is in turn encouraged by the presence of the context words transferred to, which indicate an employment relation. previous joint models have employed feature-based structured learning. an alternative approach to this end-to-end relation extraction task is to employ automatic feature learning via neural network based models.there are two ways to represent relations between entities using neural networks: recurrent/recursive neural networks and convolutional neural networks . among these, rnns can directly represent essential linguistic structures, i.e., word sequences and constituent/dependency trees. despite this representation ability, for relation classification tasks, the previously reported performance using long short-term memory based rnns is worse than one using cnns. these previous lstm-based systems mostly include limited linguistic structures and neural architectures, and do not model entities and relations jointly. we are able to achieve improvements over state-of-the-art models via endto-end modeling of entities and relations based on richer lstm-rnn architectures that incorporate complementary linguistic structures.word sequence and tree structure are known to be complementary information for extracting relations. for instance, dependencies between words are not enough to predict that source and u.s. have an org-aff relation in the sentence \"this is ...\", one u.s. source said, and the context word said is required for this prediction. many traditional, feature-based relation classification models extract features from both sequences and parse trees. however, previous rnnbased models focus on only one of these linguistic structures.we present a novel end-to-end model to extract relations between entities on both word sequence and dependency tree structures. our model allows joint modeling of entities and relations in a single model by using both bidirectional sequential and bidirectional tree-structured lstm-rnns. our model first detects entities and then extracts relations between the detected entities using a single incrementally-decoded nn structure, and the nn parameters are jointly updated using both entity and relation labels. unlike traditional incremental end-to-end relation extraction models, our model further incorporates two enhancements into training: entity pretraining, which pretrains the entity model, and scheduled sampling, which replaces predicted labels with gold labels in a certain probability. these enhancements alleviate the problem of low-performance entity detection in early stages of training, as well as allow entity information to further help downstream relation classification.on end-to-end relation extraction, we improve over the state-of-the-art feature-based model, with 12.1% and 5.7% relative error reductions in f1-score. on nominal relation classification , our model compares favorably to the state-of-the-art cnnbased model in f1-score. finally, we also ablate and compare our various model components, which leads to some key findings about the contribution and effectiveness of different rnn structures, input dependency relation structures, different parsing models, external resources, and joint learning settings.',\n",
       " 'joint extraction of entities and relations is to detect entity mentions and recognize their semantic relations simultaneously from unstructured text, as shows. different from open information extraction ) whose relation words are extracted from the given sentence, in this task, relation words are extracted from a predefined relation set which may not appear in the given sentence. it is an important issue in knowledge extraction and automatic construction of knowledge base. traditional methods handle this task in a pipelined manner, i.e., extracting the entities first and then recognizing their relations. this separated framework makes the task easy to deal with, and each component can be more flexible. but it neglects the relevance between these two sub-tasks and each subtask is an independent model. the results of entity recognition may affect the performance of relation classification and lead to erroneous delivery.different from the pipelined methods, joint learning framework is to extract entities together with relations using a single model. it can effectively integrate the information of entities and relations, and it has been shown to achieve better results in this task. however, most existing joint methods are feature-based structured systems. they need complicated feature engineering and heavily rely on the other nlp toolkits, which might also lead to error propagation. in order to reduce the manual work in feature extraction, recently, presents a neural network-based method for the end-to-end entities and relations extraction. although the joint models can represent both entities and relations with shared parameters in a single model, they also extract the entities and relations separately and produce redundant information. for instance, the sentence in contains three entities: \"united states\", \"trump\" and \"apple inc\". but only \"united states\" and \"trump\" hold a fix relation \"country-president\". entity \"apple inc\" has no obvious relationship with the other entities in this sentence. hence, the extracted result from this sentence is {united states e1 , country-president r , trump e2 }, which called triplet here.in this paper, we focus on the extraction of triplets that are composed of two entities and one relation between these two entities. therefore, we can model the triplets directly, rather than extracting the entities and relations separately. based on the motivations, we propose a tagging scheme accompanied with the end-to-end model to settle this problem. we design a kind of novel tags which contain the information of entities and the relationships they hold. based on this tagging scheme, the joint extraction of entities and relations can be transformed into a tagging problem. in this way, we can also easily use neural networks to model the task without complicated feature engineering.recently, end-to-end models based on lstm have been successfully applied to various tagging tasks: named entity recognition, ccg supertagging, chunking et al. lstm is capable of learning long-term dependencies, which is beneficial to sequence modeling tasks. therefore, based on our tagging scheme, we investigate different kinds of lstm-based end-to-end models to jointly extract the entities and relations. we also modify the decoding method by adding a biased loss to make it more suitable for our special tags.the method we proposed is a supervised learning algorithm. in reality, however, the process of manually labeling a training set with a large number of entity and relation is too expensive and error-prone. therefore, we conduct experiments on a public dataset 1 which is produced by distant supervision method to validate our approach. the experimental results show that our tagging scheme is effective in this task. in addition, our end-to-end model can achieve the best results on the public dataset.the major contributions of this paper are: a novel tagging scheme is proposed to jointly extract entities and relations, which can easily transform the extraction problem into a tagging task. based on our tagging scheme, we study different kinds of end-to-end models to settle the problem. the tagging-based methods are better than most of the existing pipelined and joint learning methods. furthermore, we also develop an end-to-1 link end model with biased loss function to suit for the novel tags. it can enhance the association between related entities.',\n",
       " \"the goal of the entity recognition and relation extraction is to discover relational structures of entity mentions from unstructured texts. it is a central problem in information extraction since it is critical for tasks such as knowledge base population and question answering.the problem is traditionally approached as two separate subtasks, namely named entity recognition and relation extraction , in a pipeline setting. the main limitations of the pipeline models are: error propagation between the components and possible useful information from the one task is not exploited by the other . on the other hand, more recent studies propose to use joint models to detect entities and their relations overcoming the aforementioned issues and achieving state-of-the-art performance.the previous joint models heavily rely on hand-crafted features. recent advances in neural networks alleviate the issue of manual feature engineering, but some of them still depend on nlp tools . propose a recurrent neural network -based joint model that uses a bidirectional sequential lstm to model the entities and a tree-lstm that takes into account dependency tree information to model the relations between the entities. the dependency information is extracted using an external dependency parser. similarly, in the work of for entity and relation extraction from biomedical text, a model which also uses tree-lstms is applied to extract dependency information. propose a method that relies on rnns but uses a lot of hand-crafted features and additional nlp tools to extract features such as pos-tags, etc. replicate the context around the entities with convolutional neural networks . note that the aforementioned works examine pairs of entities for relation extraction, rather than modeling the whole sentence directly. this means that relations of other pairs of entities in the same sentence -which could be helpful in deciding on the relation type fora particular pair -are not taken into account. propose a neural joint model based on lstms where they model the whole sentence at once, but still they do not have a principled way to deal with multiple relations. introduce a quadratic scoring layer to model the two tasks simultaneously. the limitation of this approach is that only a single relation can be assigned to a token, while the time complexity for the entity recognition task is increased compared to the standard approaches with linear complexity.in this work, we focus on anew general purpose joint model that performs the two tasks of entity recognition and relation extraction simultaneously, and that can handle multiple relations together. our model achieves state-of-the-art performance in a number of different contexts and languages without relying on any manually engineered features nor additional nlp tools. in summary, our proposed model solves several shortcomings that we identified in related works for joint entity recognition and relation extraction: our model does not rely on external nlp tools nor hand-crafted features, entities and relations within the same text fragment are extracted simultaneously, where an entity can be involved in multiple relations at once. specifically, the model of depends on dependency parsers, which perform particularly well on specific languages and contexts . yet, our ambition is to develop a model that generalizes well in various setups, therefore using only automatically extracted features that are learned during training.for instance, and use exactly the same model in different contexts, i.e., news and biomedical data , respectively. comparing our results to the ade dataset, we obtain a 1.8% improvement on the ner task and ?3% on the re task. on the other hand, our model performs within a reasonable margin on the ace04 dataset without the use of pre-calculated features. this shows that the model of strongly relies on the features extracted by the dependency parsers and cannot generalize well into different contexts where dependency parser features are weak.comparing to, we train our model by modeling all the entities and the relations of the sentence at once. this type of inference is beneficial in obtaining information about neighboring entities and relations instead of just examining a pair of entities each time. finally, we solve the underlying problem of the models proposed by and, who essentially assume classes to be mutually exclusive: we solve this by phrasing the relation extraction component as a multi-label prediction problem. to demonstrate the effectiveness of the proposed method, we conduct the largest experimental evaluation to date in jointly performing both entity recognition and relation extraction , using different datasets from various domains and languages . specifically, we apply our method to four datasets, namely ace04 , adverse drug events , dutch real estate classifieds and conll'04 . our method outperforms all state-of-the-art methods that do not rely on any additional features or tools, while performance is very close compared to methods that do exploit hand-engineered features or nlp tools.\",\n",
       " 'many neural network methods have recently been exploited in various natural language processing tasks, such as parsing , pos tagging, relation extraction, translation, and joint tasks. however, observed that intentional small scale perturbations to the input of such models may lead to incorrect decisions . proposed adversarial training as a regularization method which uses a mixture of clean and adversarial examples to enhance the robustness of the model. although at has recently been applied in nlp tasks , this paper -to the best of our knowledge -is the first attempt investigating regularization effects of at in a joint setting for two related tasks.we start from a baseline joint model that performs the tasks of named entity recognition and relation extraction at once. previously proposed models exhibit several issues that the neural network-based baseline approach overcomes: our model uses automatically extracted features without the need of external parsers nor manually extracted features , all entities and the corresponding relations within the sentence are extracted at once, instead of examining one pair of entities at a time , and we model relation extraction in a multi-label setting, allowing multiple relations per entity . the core contribution of the paper is the use of at as an extension in the training procedure for the joint extraction task .to evaluate the proposed at method, we perform a large scale experimental study in this joint task , using datasets from different contexts and languages . we use a strong baseline that outperforms all previous models that rely on automatically extracted features, achieving state-of-the-art performance . compared to the baseline model, applying at during training leads to a consistent additional increase in joint extraction effectiveness.',\n",
       " 'relation extraction involves discerning whether a relation exists between two entities in a sentence . successful relation extraction is the cornerstone of applications requiring relational understanding of unstructured text on a large scale, such as question answering, knowledge base population, and biomedical knowledge discovery.models making use of dependency parses of the input sentences, or dependency-based models,: an example modified from the tac kbp challenge corpus. a subtree of the original ud dependency tree between the subject and object is also shown, where the shortest dependency path between the entities is highlighted in bold. note that negation is off the dependency path.have proven to be very effective in relation extraction, because they capture long-range syntactic relations that are obscure from the surface form alone . traditional feature-based models are able to represent dependency information by featurizing dependency trees as overlapping paths along the trees. however, these models face the challenge of sparse feature spaces and are brittle to lexical variations. more recent neural models address this problem with distributed representations built from their computation graphs formed along parse trees. one common approach to leverage dependency information is to perform bottom-up or top-down computation along the parse tree or the subtree below the lowest common ancestor of the entities. another popular approach, inspired by, is to reduce the parse tree to the shortest dependency path between the entities. however, these models suffer from several drawbacks. neural models operating directly on parse trees are usually difficult to parallelize and thus computationally inefficient, because aligning trees for efficient batch training is usually nontrivial. models based on the shortest dependency path between the subject and object are computationally more efficient, but this simplifying assumption has major limitations as well. shows a real-world example where crucial information would be excluded when the model is restricted to only considering the dependency path.in this work, we propose a novel extension of the graph convolutional network) that is tailored for relation extraction. our model encodes the dependency structure over the input sentence with efficient graph convolution operations, then extracts entity-centric representations to make robust relation predictions. we also apply a novel path-centric pruning technique to remove irrelevant information from the tree while maximally keeping relevant content, which further improves the performance of several dependencybased models including ours. we test our model on the popular semeval 2010 task 8 dataset and the more recent, larger tac-red dataset. on both datasets, our model not only outperforms existing dependency-based neural models by a significant margin when combined with the new pruning technique, but also achieves a 10-100x speedup over existing tree-based models. on tacred, our model further achieves the state-of-the-art performance, surpassing a competitive neural sequence model baseline. this model also exhibits complementary strengths to sequence models on tacred, and combining these two model types through simple prediction interpolation further improves the state of the art.to recap, our main contributions are: we propose a neural model for relation extraction based on graph convolutional networks, which allows it to efficiently pool information over arbitrary dependency structures; we present anew pathcentric pruning technique to help dependencybased models maximally remove irrelevant information without damaging crucial content to improve their robustness; we present detailed analysis on the model and the pruning technique, and show that dependency-based models have complementary strengths with sequence models.',\n",
       " 'extracting entities and their semantic relations from raw text is a key information extraction task. for example, given the sentence \" david foster is the ap \\'s northwest regional reporter , based in seattle \" in the conll04 dataset, our goal is to recognize \"david foster\" as person, \"ap\" as organization, and \"northwest\" and \"seattle\" as location entities, then classifiy entity pairs to extract structured information: work for, orgbased in and orgbased in. such information is useful in many other nlp tasks. especially in ir applications such as entity search, structured search and question answering, it helps provide end users with significantly better search experience.a common relation extraction approach is to construct pipeline systems with separate sub-systems for the two tasks of named entity recognition and relation classification. more recently, end-to-end systems which jointly learn to extract entities and relations have been proposed with strong potential to obtain high performance. traditional joint approaches are feature-based supervised learning methods which employ numerous syntactic and lexical features based on external nlp tools as well as knowledge base resources.state-of-the-art relation extraction performance has been obtained by end-to-end models based on neural networks. specifically, proposed a rnnbased model which achieved top results on the conll04 dataset. their approach relies on various manually extracted features. other neural models employ dependency parsing-based information. in particular, applied bottom-up and top-down tree-structured lstms to model dependency paths between entities. integrated implicit syntactic information by using latent feature representations extracted from a pre-trained bilstm-based dependency parser. entity recognition, and a cnn on top of the bilstm for classifying relations. adel and sch tze assumed that entity boundaries are given, and trained a cnn to extract context features around the entities, and using these features for entity and relation classification. recently, formulated the joint entity and relation extraction problem as a directed graph and proposed a bilstm-and transition-based approach to generate the graph incrementally. extended the multi-head selection-based joint model with adversarial training. in, the joint task is formulated as a sequence tagging problem, and a bilstm with a softmax output layer can then be used for joint prediction.in this paper, we present a novel end-to-end neural model for joint entity and relation extraction. as illustrated in, our model architecture can be viewed as a mixture of a named entity recognition component and a relation classification component. our ner component employs a bilstm-crf architecture to predict entities from input word tokens. based on both the input words and the predicted ner labels, the rc component uses another bilstm to learn latent features relevant for relation classification. in most previous neural joint models, the relation classification part relies on a common \"linear\" concatenation-based mechanism over the latent features associated with entity pairs, i.e. the latent features are first concatenated into a single feature vector which is then linearly transformed before being fed into a softmax classifier. in contrast, our rc component takes into account second-order interactions over the latent features via a tensor. in particular, for relation classification we propose a novel use of the deep biaffine attention mechanism which was first introduced in dependency parsing.experimental results on the benchmark \"relation and entity recognition\" dataset conll04 show that our model outperforms previous models, obtaining new stateof-the-art scores. in addition, using the biaffine attention improves the performance compared to using the linear mechanism significantly. we also provide an ablation study to investigate effects of different contributing factors in our model.',\n",
       " \"classifying semantic relations between entity pairs in sentences plays a vital role in various nlp tasks, such as information extraction, question answering and knowledge base population. a task of relation classification is defined as predicting a semantic relationship between two tagged entities in a sentence. for example, given a sentence with tagged entity pair, crash and attack, this sentence is classified into the re-lation cause-effect 1 between the entity pair like. a first entity is surrounded by e1 and /e1 , and a second entity is surrounded by e2 and /e2 .most previous relation classification models rely heavily on high-level lexical and syntactic features obtained from nlp tools such as wordnet, dependency parser, part-of-speech tagger, and named entity recognizer . the classification models relying on such features suffer from propagation of implicit error of the tools and they are computationally expensive.recently, many studies therefore propose end-toend neural models without the high-level features. among them, attention-based models, which focus to the most important semantic information in a sentence, show state-of-the-art results in a lot of nlp tasks. since these models are mainly proposed for solving translation and language modeling tasks, they could not fully utilize the information of tagged entities in relation classification task. however, tagged entity pairs could be powerful hints for solving relation classification task. for example, even if we do not consider other words except the crash and attack, we intuitively know that the entity pair has a relation cause-effect 1 better than component-whole 1 in to address these issues, we propose a novel endto-end recurrent neural model which incorporates an entity-aware attention mechanism with a latent entity typing . to capture the context of sentences, we obtain word representations by self attention mechanisms and build the recurrent neural architecture with bidirectional long short-term memory networks. entity-aware attention focuses on the most important semantic information considering entity pairs with word positions relative to these pairs and latent types obtained by let.the contributions of our work are summarized as follows: we propose an novel end-to-end recurrent neural model and an entity-aware attention mechanism with a let which focuses to semantic information of entities and their latent types; our model obtains 85.2% f1-score in semeval-2010 task 8 and it outper- forms existing state-of-the-art models without any highlevel features; we show that our model is more interpretable since it's decision making process could be visualized with self attention, entity-aware attention, and let.\",\n",
       " 'the volume of biomedical literature continues to rapidly increase. on average, more than 3000 new articles are published everyday in peer-reviewed journals, excluding pre-prints and technical reports such as clinical trial reports in various archives. pubmed alone has a total of 29m articles as of january 2019. reports containing valuable information about new discoveries and new insights are continuously added to the already overwhelming amount of literature. consequently, there is increasingly more demand for accurate biomedical text mining tools for extracting information from the literature.recent progress of biomedical text mining models was made possible by the advancements of deep learning techniques used in natural language processing . for instance, long short-term memory and conditional random field have greatly improved performance in biomedical named entity recognition over the last few years. other deep learning based models have made improvements in biomedical text mining tasks such as relation extraction and question answering .however, directly applying state-of-the-art nlp methodologies to biomedical text mining has limitations. first, as recent word representation models such as word2vec, elmo and bert are trained and tested mainly on datasets containing general domain texts , it is difficult to estimate their performance on datasets containing biomedical texts. also, the word distributions of general and biomedical corpora are quite different, which can often be a problem for biomedical text mining models. as a result, recent models in biomedical text mining rely largely on adapted versions of word representations.in this study, we hypothesize that current state-of-the-art word representation models such as bert need to be trained on biomedical corpora to be effective in biomedical text mining tasks. previously, word2vec, which is one of the most widely known context independent word representation models, was trained on biomedical corpora which contain terms and expressions that are usually not included in a general domain corpus. while elmo and bert have proven the effectiveness of contextualized word representations, they cannot obtain high performance on biomedical corpora because they are pre-trained on only general domain corpora. as bert achieves very strong results on various nlp tasks while using almost the same structure across the tasks, adapting bert for the biomedical domain could potentially benefit numerous biomedical nlp researches.',\n",
       " 'relation extraction aims to find the semantic relation between a pair of entity mentions from an input paragraph. a solution to this task is essential for many downstream nlp applications such as automatic knowledge-base completion, knowledge base question answering, and symbolic approaches for visual question answering, etc.one particular type of the re task is multiplerelations extraction that aims to recognize relations of multiple pairs of entity mentions from an input paragraph. because in real-world applications, whose input paragraphs dominantly contain multiple pairs of entities, an efficient and effective solution for mre has more important and more practical implications. however, nearly all existing approaches for mre tasks 2014; adopt some variations of the singlerelation extraction approach, which treats each pair of entity mentions as an independent instance, and requires multiple passes of encoding for the multiple pairs of entities. the drawback of this approach is obvious -it is computationally expensive and this issue becomes more severe when the input paragraph is large, making this solution impossible to implement when the encoding step involves deep models.this work presents a solution that can resolve the inefficient multiple-passes issue of existing solutions for mre by encoding the input only once, which significantly increases the efficiency and scalability. specifically, the proposed solution is built on top of the existing transformer-based, pretrained general-purposed language encoders. in this paper we use bidirectional encoder representations from transformers as the transformer-based encoder, but this solution is not limited to using bert alone. the two novel modifications to the original bert architecture are: we introduce a structured prediction layer for predicting multiple relations for different entity pairs; and we make the selfattention layers aware of the positions of all en-tities in the input paragraph. to the best of our knowledge, this work is the first promising solution that can solve mre tasks with such high efficiency and effectiveness , as proved on the ace 2005 benchmark.',\n",
       " 'the exponential increase in the volume of scientific publications in the past decades has made nlp an essential tool for large-scale knowledge extraction and machine reading of these documents. recent progress in nlp has been driven by the adoption of deep neural models, but training such models often requires large amounts of labeled data. in general domains, large-scale training data is often possible to obtain through crowdsourcing, but in scientific domains, annotated data is difficult and expensive to collect due to the expertise required for quality annotation.as shown through elmo , and bert, unsupervised pretraining of language models on large corpora significantly improves performance on many nlp tasks. these models return contextualized embeddings for each token which can be passed into minimal task-specific neural architectures. leveraging the success of unsupervised pretraining has become especially important especially when task-specific annotations are difficult to obtain, like in scientific nlp. yet while both bert and elmo have released pretrained models, they are still trained on general domain corpora such as news articles and wikipedia.in this work, we make the following contributions: we release scibert, anew resource demonstrated to improve performance on a range of nlp tasks in the scientific domain. scibert is a pretrained language model based on bert but trained on a large corpus of scientific text. we perform extensive experimentation to investigate the performance of finetuning versus task-specific architectures atop frozen embeddings, and the effect of having an in-domain vocabulary. we evaluate scibert on a suite of tasks in the scientific domain, and achieve new state-ofthe-art results on many of these tasks.',\n",
       " \"text classification is a classic topic for natural language processing, in which one needs to assign predefined categories to free-text documents. the range of text classification research goes from designing the best features to choosing the best possible machine learning classifiers. to date, almost all techniques of text classification are based on words, in which simple statistics of some ordered word combinations usually perform the best.on the other hand, many researchers have found convolutional networks are useful in extracting information from raw signals, ranging from computer vision applications to speech recognition and others. in particular, time-delay networks used in the early days of deep learning research are essentially convolutional networks that model sequential data.in this article we explore treating text as a kind of raw signal at character level, and applying temporal convnets to it. for this article we only used a classification task as away to exemplify convnets' ability to understand texts. historically we know that convnets usually require large-scale datasets to work, therefore we also build several of them. an extensive set of comparisons is offered with traditional models and other deep learning models.applying convolutional networks to text classification or natural language processing at large was explored in literature. it has been shown that convnets can be directly applied to distributed or discrete embedding of words, without any knowledge on the syntactic or semantic structures of a language. these approaches have been proven to be competitive to traditional models. from previous research that convnets do not require the knowledge about the syntactic or semantic structure of a language. this simplification of engineering could be crucial fora single system that can work for different languages, since characters always constitute a necessary construct regardless of whether segmentation into words is possible. working on only characters also has the advantage that abnormal character combinations such as misspellings and emoticons maybe naturally learnt.\",\n",
       " \"text categorization is the task of assigning labels to documents written in a natural language, and it has numerous real-world applications including sentiment analysis as well as traditional topic assignment tasks. the state-of-the art methods for text categorization had long been linear predictors with either bag-ofword or bag-of-n-gram vectors as input, e.g.,. this, however, a convolutional neural network ) is a feedforward neural network with convolution layers interleaved with pooling layers, originally developed for image processing. in its convolution layer, a small region of data at every location is converted to a low-dimensional vector with information relevant to the task being preserved, which we loosely term 'embedding'. the embedding function is shared among all the locations, so that useful features can be detected irrespective of their locations. in its simplest form, onehot cnn works as follows. a document is represented as a sequence of one-hot vectors ; a convolution layer converts small regions of the document to low-dimensional vectors at every location ; a pooling layer aggregates the region embedding results to a document vector by taking componentwise maximum or average; and the top layer classifies a document vector with a linear model. the onehot cnn and its semi-supervised extension were shown to be superior to a number of previous methods.in this work, we consider a more general framework which jointly trains a feature generator and a linear model, where the feature generator consists of 'region embedding + pooling'. the specific region embedding function of one-hot cnn takes the simple form v = max ,where x is a concatenation of one-hot vectors of the words in the -th region , and the weight matrix w and the bias vector b need to be trained. it is simple and fast to compute, and considering its simplicity, the method works surprisingly well if the region size is appropriately set. however, there are also potential shortcomings. the region size must be fixed, which may not be optimal as the size of relevant regions may vary. practically, the region size cannot be very large as the number of parameters to be learned depends on it. jz15 proposed variations to alleviate these issues. for example, a bow-input variation allows x above to be a bow vector of the region. this enables a larger region, but at the expense of losing word order in the region and so its use maybe limited.in this work, we build on the general framework of 'region embedding + pooling' and explore a more sophisticated region embedding via long short-term memory , seeking to overcome the shortcomings above, in the supervised and semi-supervised settings. lstm) is a recurrent neural network. in its typical applications to text, an lstm takes words in a sequence one by one; i.e., at time t, it takes as input the t-th word and the output from time t ? 1. therefore, the output from each time step can be regarded as the embedding of the sequence of words that have been seen so far . it is designed to enable learning of dependencies over larger time lags than feasible with traditional recurrent networks. that is, an lstm can be used to embed text regions of variable sizes.we pursue the best use of lstm for our purpose, and then compare the resulting model with the previous best methods including one-hot cnn and previous lstm. our strategy is to simplify the model as much as possible, including elimination of a word embedding layer routinely used to produce input to lstm. our findings are threefold. first, in the supervised setting, our simplification strategy leads to higher accuracy and faster training than previous lstm. second, accuracy can be further improved by training lstms on unlabeled data for learning useful region embeddings and using them to produce additional input. third, both our lstm models and one-hot cnn strongly outperform other methods including previous lstm. the best results are obtained by combining the two types of region embeddings trained on unlabeled data, indicating that their strengths are complementary. overall, our results show that for text categorization, embeddings of text regions, which can convey higher-level concepts than single words in isolation, are useful, and that useful region embeddings can be learned without going through word embedding learning. we report performances exceeding the previous best results on four benchmark datasets. our code and experimental details are available at link download.html.\",\n",
       " 'text classification is an important task in natural language processing with many applications, such as web search, information retrieval, ranking and document classification.recently, models based on neural networks have become increasingly popular. while these models achieve very good performance in practice, they tend to be relatively slow both at train and test time, limiting their use on very large datasets.meanwhile, linear classifiers are often considered as strong baselines for text classification problems. despite their simplicity, they often obtain stateof-the-art performances if the right features are used.they also have the potential to scale to very large corpus.in this work, we explore ways to scale these baselines to very large corpus with a large output space, in the context of text classification. inspired by the recent work in efficient word representation learning, we show that linear models with a rank constraint and a fast loss approximation can train on a billion words within ten minutes, while achieving performance on par with the state-of-the-art. we evaluate the quality of our approach fasttext 1 on two different tasks, namely tag prediction and sentiment analysis.',\n",
       " \"words are often considered as the basic constituents of texts for many languages, including english. the first module in an nlp pipeline is a tokenizer which transforms texts to sequences of words.however, in practise, other preprocessing techniques can be further used together with tokenization. these include lemmatization, lowercasing and 1 note that although word-based models are mainstream in nlp in general and text classification in particular, recent work has also considered other linguistic units, such as characters or word senses. these techniques require a different kind of preprocessing and, while they have been shown effective in various settings, in this work we only focus on the mainstream word-based models. multiword grouping, among others. although these preprocessing decisions have been studied in the context of conventional text classification techniques, little attention has been paid to them in the more recent neural-based models. the most similar study to ours is, which analyzed different encoding levels for english and asian languages such as chinese, japanese and korean. as opposed to our work, their analysis was focused on utf-8 bytes, characters, words, romanized characters and romanized words as encoding levels, rather than the preprocessing techniques analyzed in this paper.additionally, word embeddings have been shown to play an important role in boosting the generalization capabilities of neural systems. however, while some studies have focused on intrinsically analyzing the role of lemmatization in their underlying training corpus, the impact on their extrinsic performance when integrated into a neural network architecture has remained understudied. in this paper we focus on the role of preprocessing the input text, particularly in how it is split into individual tokens and how it affects the performance of standard neural text classification models based on. cnns have proven to be effective in a wide range of nlp applications, in-cluding text classification tasks such as topic categorization and polarity detection, which are the tasks considered in this work. the goal of our evaluation study is to find answers to the following two questions:1. are neural network architectures affected by seemingly small preprocessing decisions in the input text?2. does the preprocessing of the embeddings' underlying training corpus have an impact on the final performance of a state-of-the-art neural network text classifier?according to our experiments in topic categorization and polarity detection, these decisions are important in certain cases. moreover, we shed some light on the motivations of each preprocessing decision and provide some hints on how to normalize the input corpus to better suit each setting.the accompanying materials of this submission can be downloaded at the following repository: github.com/pedrada88/preproc-textclassification.\",\n",
       " 'in the last few years, convolutional neural networks have demonstrated remarkable progress in various natural language processing applications, including sentence/document classification, text sequence matching, generic text representations, language modeling , machine translation and abstractive sentence summarization. cnns are typically applied to tasks where feature extrac-tion and a corresponding supervised task are approached jointly. as an encoder network for text, cnns typically convolve a set of filters, of window size n, with an inputsentence embedding matrix obtained via word2vec or glove. different filter sizes n maybe used within the same model, exploiting meaningful semantic features from different n-gram fragments.the learned weights of cnn filters, inmost cases, are assumed to be fixed regardless of the input text. as a result, the rich contextual information inherent in natural language sequences may not be fully captured. as demonstrated in, the context of a word tends to greatly influence its contribution to the final supervised tasks. this observation is consistent with the following intuition: when reading different types of documents, e.g., academic papers or newspaper articles, people tend to adopt distinct strategies for better and more effective understanding, leveraging the fact that the same words or phrases may have different meaning or imply different things, depending on context.several research efforts have sought to incorporate contextual information into cnns to adaptively extract text representations. one common strategy is the attention mechanism, which is typically employed on top of a cnn ) layer to guide the extraction of semantic features. for the embedding of a single sentence, proposed a selfattentive model that attends to different parts of a sentence and combines them into multiple vector representations. however, their model needs considerably more parameters to achieve performance gains over traditional cnns. to match sentence pairs, introduced an attentionbased cnn model, which re-weights the convolution inputs or outputs, to extract interdepen-dent sentence representations.; explore a compare and aggregate framework to directly capture the wordby-word matching between two paired sentences. however, these approaches suffer from the problem of high matching complexity, since a similarity matrix between pairwise words needs to be computed, and thus it is computationally inefficient or even prohibitive when applied to long sentences.in this paper, we propose a generic approach to learn context-sensitive convolutional filters for natural language understanding. in contrast to traditional cnns, the convolution operation in our framework does not have a fixed set of filters, and thus provides the network with stronger modeling flexibility and capacity. specifically, we introduce a meta network to generate a set of contextsensitive filters, conditioned on specific input sentences; these filters are adaptively applied to either the same or different text sequences. in this manner, the learned filters vary from sentence to sentence and allow for more fine-grained feature abstraction.moreover, since the generated filters in our framework can adapt to different conditional information available , they can be naturally generalized to model sentence pairs. in this regard, we propose a novel bidirectional filter generation mechanism to allow interactions between sentence pairs while constructing context-sensitive representations.we investigate the effectiveness of our adaptive context-sensitive cnn framework on several text processing tasks: ontology classification, sentiment analysis, answer sentence selection and paraphrase identification. we show that the proposed methods consistently outperforms the standard cnn and attention-based cnn baselines. our work provides anew perspective on how to incorporate contextual information into text representations, which can be combined with more sophisticated structures to achieve even better performance in the future.',\n",
       " \"inductive transfer learning has had a large impact on computer vision . applied cv models are rarely trained from scratch, but instead are fine-tuned from models that have been pretrained on imagenet, ms-coco, and other datasets.text classification is a category of natural language processing tasks with real-world applications such as spam, fraud, and bot detection, emergency response, and commercial document classification, such as for legal discovery. 1 link equal contribution. jeremy focused on the algorithm development and implementation, sebastian focused on the experiments and writing.while deep learning models have achieved state-of-the-art on many nlp tasks, these models are trained from scratch, requiring large datasets, and days to converge. research in nlp focused mostly on transductive transfer. for inductive transfer, fine-tuning pretrained word embeddings, a simple transfer technique that only targets a model's first layer, has had a large impact in practice and is used inmost state-of-the-art models. recent approaches that concatenate embeddings derived from other tasks with the input at different layers) still train the main task model from scratch and treat pretrained embeddings as fixed parameters, limiting their usefulness.in light of the benefits of pretraining, we should be able to do better than randomly initializing the remaining parameters of our models. however, inductive transfer via finetuning has been unsuccessful for nlp. first proposed finetuning a language model but require millions of in-domain documents to achieve good performance, which severely limits its applicability.we show that not the idea of lm fine-tuning but our lack of knowledge of how to train them effectively has been hindering wider adoption. lms overfit to small datasets and suffered catastrophic forgetting when fine-tuned with a classifier. compared to cv, nlp models are typically more shallow and thus require different fine-tuning methods.we propose anew method, universal language model fine-tuning that addresses these issues and enables robust inductive transfer learning for any nlp task, akin to fine-tuning imagenet models: the same 3-layer lstm architecturewith the same hyperparameters and no additions other than tuned dropout hyperparametersoutperforms highly engineered models and trans-fer learning approaches on six widely studied text classification tasks. on imdb, with 100 labeled examples, ulmfit matches the performance of training from scratch with 10 and-given 50k unlabeled examples-with 100 more data.contributions our contributions are the following: 1) we propose universal language model fine-tuning , a method that can be used to achieve cv-like transfer learning for any task for nlp. 2) we propose discriminative fine-tuning, slanted triangular learning rates, and gradual unfreezing, novel techniques to retain previous knowledge and avoid catastrophic forgetting during fine-tuning. 3) we significantly outperform the state-of-the-art on six representative text classification datasets, with an error reduction of 18-24% on the majority of datasets. 4) we show that our method enables extremely sample-efficient transfer learning and perform an extensive ablation analysis. 5) we make the pretrained models and our code available to enable wider adoption.\",\n",
       " 'limited amounts of training data are available for many nlp tasks. this presents a challenge for data hungry deep learning methods. given the high cost of annotating supervised training data, very large training sets are usually not available for most research or industry nlp tasks. many models address the problem by implicitly performing limited transfer learning through the use of pre-trained word embeddings such as those produced by word2vec or glove. however, recent work has demonstrated strong transfer task performance using pre-trained sentence level embeddings.in this paper, we present two models for producing sentence embeddings that demonstrate good transfer to a number of other of other nlp tasks. we include experiments with varying amounts of transfer task training data to illustrate the relationship between transfer task performance and training set size. we find that our sentence embeddings can be used to obtain surprisingly good task performance with remarkably little task specific training data. the sentence encoding models are made publicly available on tf hub.engineering characteristics of models used for transfer learning are an important consideration. we discuss modeling trade-offs regarding memory requirements as well as compute time on cpu and gpu. resource consumption comparisons are made for sentences of varying lengths. import tensorflow_hub as hub embed = hub.module embedding = embedlisting 1: python example code for using the universal sentence encoder.',\n",
       " 'modeling articles or sentences computationally is a fundamental topic in natural language processing. it could be as simple as a keyword/phrase matching problem, but it could also be a nontrivial problem if compositions, hierarchies, and structures of texts are considered. for example, a news article which mentions a single phrase \"us election\" maybe categorized into the political news with high probability. but it could be very difficult fora computer to predict which presidential candidate is favored by its author, or whether the author\\'s view in the article is more liberal or more conservative.earlier efforts in modeling texts have achieved limited success on text categorization using a simple bag-of-words classifier, implying understanding the meaning of the individual word or n-gram is a necessary step towards more sophisticated models. it is therefore not a surprise that distributed representations of words, a.k.a. word embeddings, have received great attention from nlp community addressing the question \"what\" to be modeled at the basic level. in order to model higher level concepts and facts in texts, an nlp researcher has to think cautiously the so-called \"what\" question: what is actually modeled beyond word meanings. a common approach to the question is to treat the texts as sequences and focus on their spatial patterns, whose representatives include convolutional neural networks and long shortterm memory networks . another common approach is to completely ignore the order of words but focus on their compositions as a collection, whose representatives include probabilistic topic modeling and earth mover\\'s distance based modeling.those two approaches, albeit quite different from the computational perspective, actually follow a common measure to be diagnosed regarding their answers to the \"what\" question. in neural network approaches, spatial patterns aggregated at lower levels contribute to representing higher level concepts. here, they form a recursive process to articulate what to be modeled. for example, cnn builds convolutional feature detectors to extract local patterns from a window of vector sequences and uses max-pooling to select the most prominent ones. it then hierarchically builds such pattern extraction pipelines at multiple levels. being a spatially sensitive model, cnn pays a price for the inefficiency of replicating feature detectors on a grid. as argued in, one has to choose between replicating detectors whose size grows exponentially with the number of dimensions, or increasing the volume of the labeled training set in a similar exponential way. on the other hand, methods that are spatially insensitive are perfectly efficient at the inference time regardless of any order of words or local patterns. however, they are unavoidably more restricted to encode rich structures presented in a sequence. improving the efficiency to encode spatial patterns while keeping the flexibility of their representation capability is thus a central issue.a recent method called capsule network introduced by possesses this attractive potential to address the aforementioned issue. they introduce an iterative routing process to decide the credit attribution between nodes from lower and higher layers. a metaphor they made is that human visual system intelligently assigns parts to wholes at the inference time without hard-coding patterns to be perspective relevant. as an outcome, their model could encode the intrinsic spatial relationship between apart and a whole constituting viewpoint invariant knowledge that automatically generalizes to novel viewpoints. in our work, we follow a similar spirit to use this technique in modeling texts. three strategies are proposed to stabilize the dynamic routing process to alleviate the disturbance of some noise capsules which may contain \"background\" information such as stop words and the words that are unrelated to specific categories. we conduct a series of experiments with capsule networks on top of the pre-trained word vectors for six text classification benchmarks. more importantly, we show that capsule networks achieves significant improvement when transferring singlelabel to multi-label text classifications over strong baseline methods.',\n",
       " \"word embeddings, learned from massive unstructured text data, are widely-adopted building blocks for natural language processing . by representing each word as a fixed-length vector, these embeddings can group semantically similar words, while implicitly encoding rich linguis-tic regularities and patterns. leveraging the word-embedding construct, many deep architectures have been proposed to model the compositionality in variable-length text sequences. these methods range from simple operations like addition, to more sophisticated compositional functions such as recurrent neural networks , convolutional neural networks and recursive neural networks.models with more expressive compositional functions, e.g., rnns or cnns, have demonstrated impressive results; however, they are typically computationally expensive, due to the need to estimate hundreds of thousands, if not millions, of parameters. in contrast, models with simple compositional functions often compute a sentence or document embedding by simply adding, or averaging, over the word embedding of each sequence element obtained via, e.g., word2vec, or glove. generally, such a simple word-embedding-based model does not explicitly account for spatial, word-order information within a text sequence. however, they possess the desirable property of having significantly fewer parameters, enjoying much faster training, relative to rnn-or cnn-based models. hence, there is a computation-vs.-expressiveness tradeoff regarding how to model the compositionality of a text sequence.in this paper, we conduct an extensive experimental investigation to understand when, and why, simple pooling strategies, operated over word embeddings alone, already carry sufficient information for natural language understanding. to ac-count for the distinct nature of various nlp tasks that may require different semantic features, we compare swem-based models with existing recurrent and convolutional networks in a pointby-point manner. specifically, we consider 17 datasets, including three distinct nlp tasks: document classification , natural language sequence matching and sentence classification/tagging do not meaningfully contribute to the final predictions . based upon this understanding, we propose to leverage a max-pooling operation directly over the word embedding matrix of a given sequence, to select its most salient features. this strategy is demonstrated to extract complementary features relative to the standard averaging operation, while resulting in a more interpretable model. inspired by a case study on sentiment analysis tasks, we further propose a hierarchical pooling strategy to abstract and preserve the spatial information in the final representations. this strategy is demonstrated to exhibit comparable empirical results to lstm and cnn on tasks that are sensitive to word-order features, while maintaining the favorable properties of not having compositional parameters, thus fast training.our work presents a simple yet strong baseline for text representation learning that is widely ignored in benchmarks, and highlights the general computation-vs.-expressiveness tradeoff associated with appropriately selecting compositional functions for distinct nlp problems. furthermore, we quantitatively show that the word-embeddingbased text classification tasks can have the similar level of difficulty regardless of the employed models, using the subspace training to constrain the trainable parameters. thus, according to occam's razor, simple models are preferred.\",\n",
       " 'one of the primary tasks in natural language processing is sentence classification, where given a sentence as input, we are tasked to classify it into one of multiple classes . this task is important as it is widely used in almost all subareas of nlp such as sentiment classification for sentiment analysis and question type classification for question answering, to name a few. while past methods require feature engineering, recent methods enjoy neural-based methods to automatically encode the sentences into low-dimensional dense vectors. despite the success of these methods, the major challenge in this task is that extracting features from a single sentence limits the performance.to overcome this limitation, recent works attempted to augment different kinds of features to the sentence, such as the neighboring sentences and the topics of the sentences. however, these methods used domain-dependent contexts that are only effective when the domain of the task is appropriate. for one thing, neighboring sentences may not be available in some tasks such as question type classification. moreover, topics inferred using topic models may produce less useful topics when the data set is domain-specific such as movie review sentiment classification.in this paper, we propose the usage of translations as compelling and effective domain-free contexts, or contexts that are always available no matter what the task domain is. we observe two opportunities when using translations.first, each language has its own linguistic and cultural characteristics that may contain different signals to effectively classify a specific class. contrasts the sentence vectors of the original english sentences and their arabictranslated sentences in the question type classification task. a yellow circle signifies a clear separation of a class. for example, the green class, or the numeric question type, is circled in the arabic space as it is clearly separated from other classes, while such separation cannot be observed in english. meanwhile, location type questions are better classified in english.second, the original sentences may include languagespecific ambiguity, which maybe resolved when presented with its translations. consider the example english sentence \"the movie is terribly amazing\" for the sentiment classification task. in this case, terribly can be used in both positive and negative sense, thus introduces ambiguity in the sentence. when translated to korean, it becomes \"? ? ?? ?? ? ? ? ?? ? ?? ? ? ? ? ? ? ?? ? ?? ?? ?\" which means \"the movie is greatly magnificent\", removing the ambiguity.the above two observations hold only when translations are supported for arbitrary language pairs with sufficiently high quality. thankfully, translation services moreover, recent research on neural machine translation improved the efficiency and even enabled zero-shot translation of models for languages with no parallel data. this provides an opportunity to leverage on as many languages as possible to any domain, providing a much wider context compared to the limited contexts provided by past studies.however, despite the maturity of translation, naively concatenating their vectors to the original sentence vector may introduce more noise than signals. the unaltered translation space on the left of shows an example where translation noises make the two classes indistinguishable.in this paper, we propose a method to mitigate the possible problems when using translated sentences as context based on the following observations. suppose there are two translated sentences a and b with slight errors. we posit that a can be used to fix b when a is used as a context of b, and vice versa 1 . revisiting the example above, to fix the vector of the english sentence \"the movie is terribly amazing\", we use the korean translation to move the vector towards the location where the vector \"the movie is greatly magnificent\" is.based on these observations, we present a neural attentionbased multiple context fixing attachment . mcfa is a series of modules that uses all the sentence vectors as context to fix a sentence vector . fixing the vectors is done by selectively moving the vectors to a location in the same vector space that better separates the class, as shown in. noises from translation may cause adverse effects to the vector itself and relatively to other vectors . mcfa computes two sentence usability metrics to control the noise when fixing vectors: self usability ? i weighs the confidence of using sentence a in solving the task. relative usability ? r weighs the confidence of using sentence a in fixing sentence b.listed below are the three main strengths of the mcfa attachment. mcfa is attached after encoding the sentence, which makes it widely adaptable to other models. mcfa is extensible and improves the accuracy as the number of translated sentences increases. mcfa moves the vectors inside the same space, thus preserves the meaning of vector dimensions. results show that a convolutional neural network attached with mcfa significantly improves the classification performance of cnn, achieving state of the 1 hereon, we mean to \"fix\" as to \"correct, repair, or alter.\" art performance over multiple data sets.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intros = [preprocessing_pipeline(intro) for intro in intros]\n",
    "intros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-27T04:42:11.418155Z",
     "start_time": "2022-05-27T04:42:11.391118Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "intros = pd.DataFrame(intros)\n",
    "intros.to_csv(\"intros.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
